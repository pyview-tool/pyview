"""
PyView í†µí•© ë¶„ì„ ì—”ì§„

pydeps ëª¨ë“ˆ ë ˆë²¨ ë¶„ì„ê³¼ AST ê¸°ë°˜ í´ë˜ìŠ¤/ë©”ì†Œë“œ/í•„ë“œ ë¶„ì„ì„ ê²°í•©í•˜ì—¬
ì™„ì „í•œ 5ë‹¨ê³„ ì˜ì¡´ì„± ë¶„ì„ì„ ì œê³µ
"""

import os
import sys
import uuid
import time
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Callable
from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor, as_completed

DEBUG_MODE = os.getenv('PYVIEW_DEBUG', 'false').lower() == 'true'

from .models import (
    AnalysisResult, ProjectInfo, DependencyGraph,
    PackageInfo, ModuleInfo, ClassInfo, MethodInfo,
    Relationship, CyclicDependency, QualityMetrics, EntityType,
    create_module_id
)
from .ast_analyzer import ASTAnalyzer, FileAnalysis
from .legacy_bridge import LegacyBridge
from .code_metrics import CodeMetricsEngine
from .cache_manager import CacheManager, IncrementalAnalyzer, AnalysisCache, FileMetadata
from .performance_optimizer import LargeProjectAnalyzer, PerformanceConfig, ResultPaginator
from .gitignore_patterns import create_gitignore_matcher

logger = logging.getLogger(__name__)


class AnalysisOptions:
    """ë¶„ì„ì„ ìœ„í•œ ì„¤ì • ì˜µì…˜ë“¤"""
    
    def __init__(self,
                 max_depth: int = 0,                                                           # ì˜ì¡´ì„± íƒìƒ‰ ìµœëŒ€ ê¹Šì´ (0ì´ë©´ ë¬´ì œí•œ)
                 exclude_patterns: List[str] = None,                                          # ë¶„ì„ì—ì„œ ì œì™¸í•  íŒ¨í„´ë“¤
                 include_stdlib: bool = False,                                                # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨ ì—¬ë¶€
                 analysis_levels: List[str] = None,                                           # ë¶„ì„í•  ë ˆë²¨ë“¤ (package, module, class, method, field)
                 enable_type_inference: bool = True,                                          # íƒ€ì… ì¶”ë¡  í™œì„±í™” ì—¬ë¶€
                 max_workers: int = None,                                                     # ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì›Œì»¤ ìˆ˜
                 enable_caching: bool = True,                                                 # ìºì‹± ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
                 enable_quality_metrics: bool = True,                                        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° í™œì„±í™” ì—¬ë¶€
                 enable_performance_optimization: bool = True,                               # ì„±ëŠ¥ ìµœì í™” ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
                 max_memory_mb: int = 1024):                                                  # ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)

        self.max_depth = max_depth                                                           # ì˜ì¡´ì„± íƒìƒ‰ ê¹Šì´ ì„¤ì •
        self.exclude_patterns = exclude_patterns or ['__pycache__', '.git', '.venv', 'venv', 'env', 'tests']  # ê¸°ë³¸ ì œì™¸ íŒ¨í„´ë“¤
        self.include_stdlib = include_stdlib                                                 # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨ ì„¤ì •
        self.analysis_levels = analysis_levels or ['package', 'module', 'class', 'method', 'field']  # 5ë‹¨ê³„ ë¶„ì„ ë ˆë²¨
        self.enable_type_inference = enable_type_inference                                   # íƒ€ì… ì¶”ë¡  ê¸°ëŠ¥ ì„¤ì •
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)               # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜ ì„¤ì •
        self.enable_caching = enable_caching                                                 # ìºì‹± ê¸°ëŠ¥ ì„¤ì •
        self.enable_quality_metrics = enable_quality_metrics                                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° ì„¤ì •
        self.enable_performance_optimization = enable_performance_optimization              # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.max_memory_mb = max_memory_mb                                                   # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ ì„¤ì •


class ProgressCallback:
    """ë¶„ì„ ì§„í–‰ ìƒí™©ì„ ë°›ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, callback: Callable[[dict], None] = None):
        """ì§„í–‰ë¥  ì½œë°± ì´ˆê¸°í™”"""
        self.callback = callback or self._default_callback                                   # ì‚¬ìš©ì ì •ì˜ ì½œë°± ë˜ëŠ” ê¸°ë³¸ ì½œë°± ì‚¬ìš©

    def update(self, stage: str, progress: float, **kwargs):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        data = {
            'stage': stage,                                                                  # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë‹¨ê³„
            'progress': progress,                                                            # ì§„í–‰ë¥  (0-100)
            **kwargs                                                                         # ì¶”ê°€ ì •ë³´ë“¤
        }
        self.callback(data)                                                                  # ì½œë°± í•¨ìˆ˜ í˜¸ì¶œ

    def _default_callback(self, data: dict):
        """ì½˜ì†”ì— ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” ê¸°ë³¸ ì½œë°±"""
        stage = data.get('stage', 'Unknown')                                                 # ë‹¨ê³„ëª… ì¶”ì¶œ
        progress = data.get('progress', 0)                                                   # ì§„í–‰ë¥  ì¶”ì¶œ
        logger.info(f"{stage}: {progress:.1f}%")                                             # ë¡œê·¸ ì¶œë ¥


class AnalyzerEngine:
    """ëª¨ë“  ë¶„ì„ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°ìœ¨í•˜ëŠ” ë©”ì¸ ë¶„ì„ ì—”ì§„"""

    def __init__(self, options: AnalysisOptions = None):
        """ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”"""
        self.options = options or AnalysisOptions()                                          # ë¶„ì„ ì˜µì…˜ ì„¤ì • (ê¸°ë³¸ê°’ ë˜ëŠ” ì‚¬ìš©ì ì§€ì •)
        self.logger = logging.getLogger(__name__)                                            # ë¡œê±° ì´ˆê¸°í™”

        # í•µì‹¬ ë¶„ì„ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        self.ast_analyzer = ASTAnalyzer(enable_type_inference=self.options.enable_type_inference)  # AST ê¸°ë°˜ ìƒì„¸ ë¶„ì„ê¸°
        self.legacy_bridge = LegacyBridge()                                                  # pydeps ì—°ë™ ë¸Œë¦¬ì§€
        self.metrics_engine = CodeMetricsEngine() if self.options.enable_quality_metrics else None  # ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì—”ì§„
        self.cache_manager = CacheManager() if options and options.enable_caching else None  # ë¶„ì„ ê²°ê³¼ ìºì‹œ ê´€ë¦¬ì
        self.incremental_analyzer = IncrementalAnalyzer(self.cache_manager) if self.cache_manager else None  # ì¦ë¶„ ë¶„ì„ê¸°

        # ì„±ëŠ¥ ìµœì í™” ì»´í¬ë„ŒíŠ¸ë“¤
        if options and options.enable_performance_optimization:                              # ì„±ëŠ¥ ìµœì í™”ê°€ í™œì„±í™”ëœ ê²½ìš°
            perf_config = PerformanceConfig(                                                 # ì„±ëŠ¥ ì„¤ì • ìƒì„±
                max_memory_mb=options.max_memory_mb,                                         # ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                max_workers=options.max_workers,                                             # ìµœëŒ€ ì›Œì»¤ ìˆ˜
                batch_size=100,                                                              # ë°°ì¹˜ í¬ê¸°
                enable_streaming=True,                                                       # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í™œì„±í™”
                enable_gc=True                                                               # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í™œì„±í™”
            )
            self.large_project_analyzer = LargeProjectAnalyzer(perf_config)                  # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ê¸°
            self.result_paginator = ResultPaginator()                                        # ê²°ê³¼ í˜ì´ì§• ì²˜ë¦¬ê¸°
        else:
            self.large_project_analyzer = None                                               # ì„±ëŠ¥ ìµœì í™” ë¹„í™œì„±í™”
            self.result_paginator = None

        # ë¶„ì„ ìƒíƒœ ê´€ë¦¬
        self.current_analysis_id: Optional[str] = None                                       # í˜„ì¬ ë¶„ì„ ì„¸ì…˜ ID
        self.total_files = 0                                                                 # ì „ì²´ íŒŒì¼ ìˆ˜
        self.processed_files = 0                                                             # ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜
    
    def analyze_project(self,
                       project_path: str,
                       progress_callback: ProgressCallback = None) -> AnalysisResult:
        """
        Python í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì™„ì „í•œ 5ë‹¨ê³„ ë¶„ì„ ìˆ˜í–‰

        Args:
            project_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì„ íƒì  ì½œë°±

        Returns:
            5ë‹¨ê³„ ëª¨ë“  ë ˆë²¨ì´ í¬í•¨ëœ ì™„ì „í•œ ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()                        # ë¶„ì„ ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ì„±ëŠ¥ ì¸¡ì •ìš©)
        self.current_analysis_id = str(uuid.uuid4())    # ê° ë¶„ì„ ì„¸ì…˜ì„ UUIDë¡œ ê³ ìœ  ì‹ë³„

        if progress_callback is None:                   # ì§„í–‰ë¥  ì½œë°±ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì½œë°± ìƒì„±
            progress_callback = ProgressCallback()

        try:
            # Stage 1: Project discovery and size estimation
            progress_callback.update("Discovering project files", 5)       # ì§„í–‰ë¥  5% - íŒŒì¼ íƒìƒ‰ ì‹œì‘
            project_files = self._discover_project_files(project_path)      # í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  Python íŒŒì¼ ìˆ˜ì§‘
            self.total_files = len(project_files)                           # ì „ì²´ íŒŒì¼ ìˆ˜ ì €ì¥ (ì§„í–‰ë¥  ê³„ì‚°ìš©)

            # Stage 1.2: Check for large project optimization
            if self.large_project_analyzer and len(project_files) > 1000:  # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ìµœì í™” ì¡°ê±´ í™•ì¸ (1000ê°œ íŒŒì¼ ì´ˆê³¼)
                progress_callback.update("Analyzing project complexity", 7) # ì§„í–‰ë¥  7% - ë³µì¡ë„ ë¶„ì„ ì‹œì‘
                project_stats = self.large_project_analyzer.estimate_project_size(project_path)  # íŒŒì¼ ìˆ˜ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ ë³µì¡ë„ ì¸¡ì •

                if project_stats['complexity'] in ['high', 'very_high']:    # ë†’ì€ ë³µì¡ë„ë©´ ìµœì í™”ëœ ë¶„ì„ ë°©ë²• ì‚¬ìš©
                    progress_callback.update("Large project detected, using optimized analysis", 10)  # ì§„í–‰ë¥  10% - ëŒ€ê·œëª¨ ë¶„ì„ ëª¨ë“œ
                    return self._analyze_large_project(project_path, project_files, progress_callback, start_time)  # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¡œ ë¶„ì„
                else:
                    progress_callback.update("Project size manageable, using standard analysis", 10)  # ì§„í–‰ë¥  10% - í‘œì¤€ ë¶„ì„ ëª¨ë“œ

            # Stage 1.5: Check for incremental analysis possibility
            cache_id = None                                                 # ìºì‹œ ID ì´ˆê¸°í™”
            if self.incremental_analyzer:                                   # ì¦ë¶„ ë¶„ì„ê¸°ê°€ í™œì„±í™”ëœ ê²½ìš°
                progress_callback.update("Checking cache validity", 8)     # ì§„í–‰ë¥  8% - ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
                cache_id = self.incremental_analyzer.can_use_incremental(   # ì´ì „ ë¶„ì„ ê²°ê³¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
                    project_path, vars(self.options)                       # í”„ë¡œì íŠ¸ ê²½ë¡œì™€ ë¶„ì„ ì˜µì…˜ìœ¼ë¡œ ìºì‹œ ê²€ì¦
                )

                if cache_id:                                                # ìœ íš¨í•œ ìºì‹œê°€ ìˆìœ¼ë©´
                    progress_callback.update("Performing incremental analysis", 10)  # ì§„í–‰ë¥  10% - ì¦ë¶„ ë¶„ì„ ì‹œì‘
                    try:
                        # Attempt incremental analysis
                        def full_analysis_fallback(path, files):           # ì¦ë¶„ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ í•¨ìˆ˜ ì •ì˜
                            return self._perform_full_analysis(path, files, progress_callback)  # ì „ì²´ ë¶„ì„ìœ¼ë¡œ í´ë°±

                        result = self.incremental_analyzer.perform_incremental_analysis(  # ì¦ë¶„ ë¶„ì„ ì‹¤í–‰
                            project_path, project_files, cache_id, full_analysis_fallback   # í”„ë¡œì íŠ¸ ê²½ë¡œ, íŒŒì¼ ëª©ë¡, ìºì‹œ ID, í´ë°± í•¨ìˆ˜ ì „ë‹¬
                        )

                        progress_callback.update("Incremental analysis complete", 100)     # ì§„í–‰ë¥  100% - ì¦ë¶„ ë¶„ì„ ì™„ë£Œ
                        return result                                                       # ì¦ë¶„ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

                    except Exception as e:                                                  # ì¦ë¶„ ë¶„ì„ ì‹¤íŒ¨ ì‹œ
                        self.logger.warning(f"Incremental analysis failed, falling back to full: {e}")  # ê²½ê³  ë¡œê·¸ ì¶œë ¥
                        # ì „ì²´ ë¶„ì„ìœ¼ë¡œ ê³„ì† ì§„í–‰

            # ì „ì²´ ë¶„ì„ ìˆ˜í–‰
            analysis_result = self._perform_full_analysis(project_path, project_files, progress_callback, start_time)  # ì „ì²´ ë¶„ì„ ì‹¤í–‰

            # Stage 7: Save to cache if caching enabled
            if self.cache_manager and not cache_id:                                        # ìºì‹œ ë§¤ë‹ˆì €ê°€ ìˆê³  ê¸°ì¡´ ìºì‹œê°€ ì—†ìœ¼ë©´
                progress_callback.update("Saving analysis cache", 99)                      # ì§„í–‰ë¥  99% - ìºì‹œ ì €ì¥ ì¤‘
                self._save_analysis_cache(project_path, project_files, analysis_result)    # ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥

            progress_callback.update("Analysis complete", 100)                             # ì§„í–‰ë¥  100% - ë¶„ì„ ì™„ë£Œ
            return analysis_result                                                          # ìµœì¢… ë¶„ì„ ê²°ê³¼ ë°˜í™˜

        except Exception as e:                                                              # ë¶„ì„ ê³¼ì •ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ
            self.logger.error(f"Analysis failed: {e}")                                     # ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
            raise                                                                           # ì˜ˆì™¸ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ë¡œ ì „ë‹¬

    def _discover_project_files(self, project_path: str) -> List[str]:
        """í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  Python íŒŒì¼ íƒìƒ‰ (.gitignore ìŠ¤íƒ€ì¼ íŒ¨í„´ ì§€ì›)"""
        python_files = []                                                                   # Python íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        # .gitignore ìŠ¤íƒ€ì¼ íŒ¨í„´ ë§¤ì²˜ ìƒì„±
        pattern_matcher = create_gitignore_matcher(self.options.exclude_patterns)

        project_path_obj = Path(project_path)                                              # í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜

        for root, dirs, files in os.walk(project_path):                                     # ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ìˆœíšŒ
            root_path = Path(root)

            # ì œì™¸í•  ë””ë ‰í† ë¦¬ í•„í„°ë§ (.gitignore ìŠ¤íƒ€ì¼ íŒ¨í„´ ë§¤ì¹­)
            dirs_to_keep = []
            for d in dirs:
                dir_path = root_path / d
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                try:
                    relative_dir_path = dir_path.relative_to(project_path_obj)
                    if not pattern_matcher.should_exclude(relative_dir_path):
                        dirs_to_keep.append(d)
                except ValueError:
                    # relative_to ì‹¤íŒ¨ì‹œ ì ˆëŒ€ ê²½ë¡œë¡œ í™•ì¸
                    if not pattern_matcher.should_exclude(dir_path):
                        dirs_to_keep.append(d)
            dirs[:] = dirs_to_keep

            for file in files:                                                              # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ í™•ì¸
                if file.endswith('.py'):                                                    # Python íŒŒì¼ë§Œ ì„ íƒ
                    file_path = os.path.join(root, file)                                   # ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±
                    file_path_obj = Path(file_path)

                    # .gitignore ìŠ¤íƒ€ì¼ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì œì™¸ ì—¬ë¶€ í™•ì¸
                    try:
                        relative_file_path = file_path_obj.relative_to(project_path_obj)
                        if not pattern_matcher.should_exclude(relative_file_path):
                            python_files.append(file_path)                                 # ìœ íš¨í•œ Python íŒŒì¼ ì¶”ê°€
                    except ValueError:
                        # relative_to ì‹¤íŒ¨ì‹œ ì ˆëŒ€ ê²½ë¡œë¡œ í™•ì¸
                        if not pattern_matcher.should_exclude(file_path_obj):
                            python_files.append(file_path)

        self.logger.info(f"Discovered {len(python_files)} Python files")                  # ë°œê²¬ëœ íŒŒì¼ ìˆ˜ ë¡œê·¸ ì¶œë ¥
        return python_files                                                                # ë°œê²¬ëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    # === ì¼ë°˜ í”„ë¡œì íŠ¸ ë¶„ì„ ê²½ë¡œ (< 1000 íŒŒì¼) ===

    def _perform_full_analysis(self, project_path: str, project_files: List[str],
                              progress_callback: ProgressCallback, start_time: float = None) -> AnalysisResult:
        """ìºì‹± ì—†ì´ ì™„ì „í•œ ë¶„ì„ ìˆ˜í–‰"""
        if start_time is None:                                                              # ì‹œì‘ ì‹œê°„ì´ ì—†ìœ¼ë©´
            start_time = time.time()                                                        # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •

        # Stage 2: pydeps module-level analysis
        progress_callback.update("Running module-level analysis", 15)                      # ì§„í–‰ë¥  15% - ëª¨ë“ˆ ìˆ˜ì¤€ ë¶„ì„ ì‹œì‘
        pydeps_result = self._run_pydeps_analysis(project_path, progress_callback)          # pydepsë¡œ ëª¨ë“ˆ ê°„ ì˜ì¡´ì„± ë¶„ì„

        # Stage 3: AST detailed analysis
        progress_callback.update("Analyzing code structure", 30)                           # ì§„í–‰ë¥  30% - ì½”ë“œ êµ¬ì¡° ë¶„ì„ ì‹œì‘
        ast_analyses = self._run_ast_analysis(project_files, progress_callback)             # ASTë¡œ ìƒì„¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„

        # Stage 4: Data integration
        progress_callback.update("Integrating analysis results", 70)                       # ì§„í–‰ë¥  70% - ë¶„ì„ ê²°ê³¼ í†µí•© ì‹œì‘
        integrated_data = self._integrate_analyses(pydeps_result, ast_analyses, progress_callback)  # pydepsì™€ AST ê²°ê³¼ í†µí•©

        # Stage 5: Quality metrics analysis
        quality_metrics = []                                                               # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        if self.metrics_engine and self.options.enable_quality_metrics:
            progress_callback.update("Calculating quality metrics", 85)
            quality_metrics = self._calculate_quality_metrics(integrated_data, project_files, progress_callback)
        else:
            progress_callback.update("Skipping quality metrics", 85)

        # Stage 6: Final result assembly
        progress_callback.update("Assembling final results", 95)                           # ì§„í–‰ë¥  95% - ìµœì¢… ê²°ê³¼ ì¡°ë¦½ ì‹œì‘
        analysis_result = self._assemble_result(                                           # ìµœì¢… ë¶„ì„ ê²°ê³¼ ì¡°ë¦½
            project_path, integrated_data, quality_metrics, start_time, progress_callback  # í”„ë¡œì íŠ¸ ê²½ë¡œ, í†µí•© ë°ì´í„°, í’ˆì§ˆ ë©”íŠ¸ë¦­, ì‹œì‘ ì‹œê°„, ì§„í–‰ ì½œë°± ì „ë‹¬
        )

        return analysis_result                                                              # ì™„ì„±ëœ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    def _run_pydeps_analysis(self, project_path: str, progress_callback: ProgressCallback) -> Dict:
        """
        pydepsë¥¼ ì‚¬ìš©í•œ ëª¨ë“ˆ ìˆ˜ì¤€ ì˜ì¡´ì„± ë¶„ì„

        Args:
            project_path: ë¶„ì„í•  í”„ë¡œì íŠ¸ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì½œë°±

        Returns:
            Dict: ëª¨ë“ˆ, íŒ¨í‚¤ì§€, ì˜ì¡´ì„± ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # PyView ì˜µì…˜ì„ pydeps í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (API ì°¨ì´ ê·¹ë³µ)                               # ê¸°ì¡´ pydeps ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í˜¸í™˜ë˜ë„ë¡ ì˜µì…˜ ë³€í™˜
            pydeps_kwargs = {
                'max_bacon': self.options.max_depth if self.options.max_depth > 0 else 999,    # ì˜ì¡´ì„± íƒìƒ‰ ê¹Šì´ (0ì´ë©´ ë¬´ì œí•œ)
                'exclude': self.options.exclude_patterns,                                      # ì œì™¸í•  íŒ¨í„´ë“¤
                'pylib': self.options.include_stdlib,                                          # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬í•¨ ì—¬ë¶€
                'verbose': 0,                                                                   # ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”
                'exclude_exact': [],                                                            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì œì™¸ íŒ¨í„´
                'noise_level': 200,                                                             # ë…¸ì´ì¦ˆ í•„í„°ë§ ë ˆë²¨
                'show_deps': True,                                                              # ì˜ì¡´ì„± í‘œì‹œ ì—¬ë¶€
                'show_cycles': True,                                                            # ìˆœí™˜ ì˜ì¡´ì„± í‘œì‹œ ì—¬ë¶€
                'max_cluster_size': 0,                                                          # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
                'min_cluster_size': 0,                                                          # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
                'keep_target_cluster': False                                                    # íƒ€ê²Ÿ í´ëŸ¬ìŠ¤í„° ìœ ì§€ ì—¬ë¶€
            }

            # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
            if DEBUG_MODE:
                print(f"ğŸ” DEBUG: include_stdlib = {self.options.include_stdlib}")
                print(f"ğŸ” DEBUG: pydeps_kwargs = {pydeps_kwargs}")
                with open('/tmp/pyview_debug.log', 'a') as f:
                    f.write(f"ğŸ” ANALYZER DEBUG: include_stdlib = {self.options.include_stdlib}, pydeps_kwargs = {pydeps_kwargs}\n")

            # pydeps ë¶„ì„ ì‹¤í–‰ (1ë‹¨ê³„: ëª¨ë“ˆ ê°„ import ê´€ê³„ ì¶”ì¶œ)                               # ê¸°ì¡´ pydepsë¡œ ëª¨ë“ˆ ë ˆë²¨ ì˜ì¡´ì„± ë¶„ì„
            dep_graph = self.legacy_bridge.analyze_with_pydeps(project_path, **pydeps_kwargs)

            # PyView ë°ì´í„° êµ¬ì¡°ë¡œ ë³€í™˜ (í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜)                                  # pydeps ê²°ê³¼ë¥¼ PyView ëª¨ë¸ì— ë§ê²Œ ë³€í™˜
            packages, modules, relationships = self.legacy_bridge.convert_pydeps_to_modules(dep_graph)

            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ìˆœí™˜ ì°¸ì¡°, ë©”íŠ¸ë¦­ ë“±)                                            # pydepsì—ì„œ ì œê³µí•˜ëŠ” ë¶€ê°€ ì •ë³´ ì¶”ì¶œ
            cycles = self.legacy_bridge.detect_cycles_from_pydeps(dep_graph)               # ëª¨ë“ˆ ìˆ˜ì¤€ ìˆœí™˜ ì°¸ì¡° íƒì§€
            metrics = self.legacy_bridge.get_pydeps_metrics(dep_graph)                     # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ì¶œ

            return {
                'dep_graph': dep_graph,                                                         # ì›ë³¸ pydeps ê·¸ë˜í”„ (ì°¸ì¡°ìš©)
                'packages': packages,                                                           # ë³€í™˜ëœ íŒ¨í‚¤ì§€ ì •ë³´
                'modules': modules,                                                             # ë³€í™˜ëœ ëª¨ë“ˆ ì •ë³´
                'relationships': relationships,                                                 # ë³€í™˜ëœ ê´€ê³„ ì •ë³´
                'cycles': cycles,                                                               # íƒì§€ëœ ìˆœí™˜ ì°¸ì¡°
                'metrics': metrics                                                              # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë°ì´í„°
            }

        except Exception as e:                                                                  # pydeps ë¶„ì„ ì‹¤íŒ¨ì‹œ
            self.logger.error(f"pydeps analysis failed: {e}")                                 # ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
            # pydeps ì‹¤íŒ¨ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (AST ë¶„ì„ë§Œìœ¼ë¡œë¼ë„ ì§„í–‰ ê°€ëŠ¥)                          # 1ë‹¨ê³„ ì‹¤íŒ¨í•´ë„ 2ë‹¨ê³„ AST ë¶„ì„ì€ ê³„ì† ì§„í–‰
            return {
                'dep_graph': None,                                                              # ê·¸ë˜í”„ ì—†ìŒ
                'packages': [],                                                                 # ë¹ˆ íŒ¨í‚¤ì§€ ë¦¬ìŠ¤íŠ¸
                'modules': [],                                                                  # ë¹ˆ ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸
                'relationships': [],                                                            # ë¹ˆ ê´€ê³„ ë¦¬ìŠ¤íŠ¸
                'cycles': [],                                                                   # ë¹ˆ ìˆœí™˜ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸
                'metrics': {}                                                                   # ë¹ˆ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            }
    
    def _run_ast_analysis(self, project_files: List[str],
                         progress_callback: ProgressCallback) -> List[FileAnalysis]:
        """ëª¨ë“  í”„ë¡œì íŠ¸ íŒŒì¼ì— ëŒ€í•´ AST ë¶„ì„ ì‹¤í–‰"""
        # ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš© ì—¬ë¶€ ê²°ì • (íŒŒì¼ì´ ë§ê³  ë©€í‹°í”„ë¡œì„¸ì‹±ì´ í™œì„±í™”ëœ ê²½ìš°)            # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë¶„ê¸° ì²˜ë¦¬
        if len(project_files) > 10 and self.options.max_workers and self.options.max_workers > 1:
            return self._run_parallel_ast_analysis(project_files, progress_callback)        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¶„ì„
        else:
            return self._run_sequential_ast_analysis(project_files, progress_callback)      # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë¶„ì„
    
    def _run_sequential_ast_analysis(self, project_files: List[str],
                                    progress_callback: ProgressCallback) -> List[FileAnalysis]:
        """ìˆœì°¨ì ìœ¼ë¡œ íŒŒì¼ì„ í•˜ë‚˜ì”© AST ë¶„ì„ (ë‹¨ì¼ ìŠ¤ë ˆë“œ)"""
        analyses = []                                                                           # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        total_files = len(project_files)                                                        # ì „ì²´ íŒŒì¼ ìˆ˜

        for i, file_path in enumerate(project_files):                                          # ê° íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
            try:
                # ê°œë³„ íŒŒì¼ ë¶„ì„ (í´ë˜ìŠ¤, ë©”ì†Œë“œ, í•„ë“œ ì¶”ì¶œ)                                    # AST íŒŒì‹±ìœ¼ë¡œ ìƒì„¸ êµ¬ì¡° ë¶„ì„
                analysis = self.ast_analyzer.analyze_file(file_path)                           # ASTAnalyzerë¡œ íŒŒì¼ ë¶„ì„
                if analysis:                                                                    # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´
                    analyses.append(analysis)                                                   # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30%ì—ì„œ ì‹œì‘í•´ì„œ 65%ê¹Œì§€)                                   # ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ë¹„ì¤‘ ë°˜ì˜
                progress_percentage = 30 + (35 * (i + 1) / total_files)
                progress_callback.update(f"Analyzing file {i+1}/{total_files}", progress_percentage)

            except Exception as e:                                                              # ê°œë³„ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ì‹œ
                self.logger.warning(f"Failed to analyze file {file_path}: {e}")               # ê²½ê³  ë¡œê·¸ (ì „ì²´ ì‹¤íŒ¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰)

        return analyses                                                                         # ëª¨ë“  íŒŒì¼ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    
    def _run_parallel_ast_analysis(self, project_files: List[str],
                                  progress_callback: ProgressCallback) -> List[FileAnalysis]:
        """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— AST ë¶„ì„ (ë©€í‹°í”„ë¡œì„¸ì‹±)"""
        analyses = []                                                                           # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        total_files = len(project_files)                                                        # ì „ì²´ íŒŒì¼ ìˆ˜
        completed_files = 0                                                                     # ì™„ë£Œëœ íŒŒì¼ ìˆ˜

        # ë©€í‹°í”„ë¡œì„¸ì‹± í’€ë¡œ ë³‘ë ¬ ì²˜ë¦¬ (CPU ì§‘ì•½ì  ì‘ì—…ì´ë¯€ë¡œ í”„ë¡œì„¸ìŠ¤ í’€ ì‚¬ìš©)               # AST íŒŒì‹±ì€ CPU ì§‘ì•½ì ì´ë¯€ë¡œ ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
        with ProcessPoolExecutor(max_workers=self.options.max_workers) as executor:
            # ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._analyze_single_file, file_path): file_path               # ê° íŒŒì¼ì„ ê°œë³„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¶„ì„
                for file_path in project_files
            }

            # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_file):                                        # ì™„ë£Œ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬
                file_path = future_to_file[future]                                             # í•´ë‹¹ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                try:
                    analysis = future.result()                                                  # ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                    if analysis:                                                                # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´
                        analyses.append(analysis)                                               # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

                except Exception as e:                                                          # ê°œë³„ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ì‹œ
                    self.logger.warning(f"Parallel analysis failed for {file_path}: {e}")     # ê²½ê³  ë¡œê·¸

                completed_files += 1                                                            # ì™„ë£Œ ì¹´ìš´í„° ì¦ê°€
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30%ì—ì„œ ì‹œì‘í•´ì„œ 65%ê¹Œì§€)                                   # ì „ì²´ ë¶„ì„ ê³¼ì •ì—ì„œì˜ ì§„í–‰ë¥  ë°˜ì˜
                progress_percentage = 30 + (35 * completed_files / total_files)
                progress_callback.update(f"Analyzing file {completed_files}/{total_files}", progress_percentage)

        return analyses                                                                         # ëª¨ë“  íŒŒì¼ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    
    @staticmethod
    def _analyze_single_file(file_path: str) -> Optional[FileAnalysis]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (ì •ì  ë©”ì†Œë“œë¡œ ë©€í‹°í”„ë¡œì„¸ì‹±ì—ì„œ ì‚¬ìš©)"""
        try:
            analyzer = ASTAnalyzer()                                                            # ìƒˆ ASTAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            return analyzer.analyze_file(file_path)                                             # íŒŒì¼ ë¶„ì„ ìˆ˜í–‰
        except Exception as e:                                                                  # ë¶„ì„ ì‹¤íŒ¨ì‹œ
            logging.getLogger(__name__).warning(f"Failed to analyze {file_path}: {e}")       # ë¡œê·¸ ì¶œë ¥
            return None                                                                         # None ë°˜í™˜
    
    def _integrate_analyses(self, pydeps_result: Dict, ast_analyses: List[FileAnalysis],
                           progress_callback: ProgressCallback) -> Dict:
        """pydepsì™€ AST ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ 5ë‹¨ê³„ ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±"""

        # 1ë‹¨ê³„: pydepsì™€ AST ê²°ê³¼ í†µí•© (ëª¨ë“ˆ-í´ë˜ìŠ¤-ë©”ì†Œë“œ-í•„ë“œ ê³„ì¸µ êµ¬ì¡° ì™„ì„±)             # 1ë‹¨ê³„(ëª¨ë“ˆ)ì™€ 2-5ë‹¨ê³„(í´ë˜ìŠ¤/ë©”ì†Œë“œ/í•„ë“œ) ì—°ê²°
        packages, modules, relationships = self.legacy_bridge.merge_with_ast_analysis(
            pydeps_result['packages'],                                                         # pydepsì—ì„œ ì¶”ì¶œí•œ íŒ¨í‚¤ì§€ ì •ë³´
            pydeps_result['modules'],                                                          # pydepsì—ì„œ ì¶”ì¶œí•œ ëª¨ë“ˆ ì •ë³´
            pydeps_result['relationships'],                                                    # pydepsì—ì„œ ì¶”ì¶œí•œ ëª¨ë“ˆ ê´€ê³„
            ast_analyses                                                                       # ASTì—ì„œ ë¶„ì„í•œ ìƒì„¸ ì •ë³´
        )

        # 2ë‹¨ê³„: AST ë¶„ì„ ê²°ê³¼ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ (í´ë˜ìŠ¤, ë©”ì†Œë“œ, í•„ë“œ)                         # ASTì—ì„œ ì¶”ì¶œí•œ ìƒì„¸ ì •ë³´ë¥¼ í‘œì¤€ ëª¨ë¸ë¡œ ë³€í™˜
        all_classes = []                                                                        # í´ë˜ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        all_methods = []                                                                        # ë©”ì†Œë“œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        all_fields = []                                                                         # í•„ë“œ ì •ë³´ ë¦¬ìŠ¤íŠ¸

        for analysis in ast_analyses:                                                           # ê° íŒŒì¼ì˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
            if analysis:                                                                        # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´
                all_classes.extend(analysis.classes)                                           # í´ë˜ìŠ¤ë“¤ì„ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                all_methods.extend(analysis.methods)                                           # ë©”ì†Œë“œë“¤ì„ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                all_fields.extend(analysis.fields)                                             # í•„ë“œë“¤ì„ ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        # 3ë‹¨ê³„: ìƒì„¸í•œ ìˆœí™˜ ì°¸ì¡° íƒì§€ (í´ë˜ìŠ¤/ë©”ì†Œë“œ ë ˆë²¨ê¹Œì§€)                                # pydeps ëª¨ë“ˆ ë ˆë²¨ ìˆœí™˜ ì°¸ì¡°ì— ë”í•´ ìƒì„¸ ë ˆë²¨ ìˆœí™˜ ì°¸ì¡° íƒì§€
        additional_cycles = self._detect_detailed_cycles(all_classes, all_methods, relationships)
        # pydeps ì‹¤íŒ¨ì‹œ AST ë¶„ì„ìœ¼ë¡œë¶€í„° import ìˆœí™˜ ì°¸ì¡° ì¶”ê°€ íƒ
        ast_import_cycles = self._detect_import_cycles_from_ast(ast_analyses)
        
        # ì§‘ê³„ëœ ModuleInfo.importsë¡œ êµ¬ì¶•í•œ ëª¨ë“ˆ ë ˆë²¨ import ìˆœí™˜ìœ¼ë¡œ ë³´ê°• 
        module_import_cycles = self._detect_import_cycles_from_modules(modules)
        all_cycles = pydeps_result['cycles'] + additional_cycles + ast_import_cycles + module_import_cycles                              # ëª¨ë“  ë ˆë²¨ì˜ ìˆœí™˜ ì°¸ì¡° í†µí•©

        # 4ë‹¨ê³„: í–¥ìƒëœ ë©”íŠ¸ë¦­ ê³„ì‚° (ëª¨ë“  ì—”í‹°í‹°ì— ëŒ€í•œ í’ˆì§ˆ ì§€í‘œ)                             # í†µí•©ëœ ë°ì´í„°ë¡œ í¬ê´„ì ì¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        enhanced_metrics = self._calculate_enhanced_metrics(
            packages, modules, all_classes, all_methods, relationships                         # ëª¨ë“  ë ˆë²¨ì˜ ì—”í‹°í‹°ì™€ ê´€ê³„ ì •ë³´
        )
        return {
            'packages': packages,                                                               # í†µí•©ëœ íŒ¨í‚¤ì§€ ì •ë³´
            'modules': modules,                                                                 # í†µí•©ëœ ëª¨ë“ˆ ì •ë³´
            'classes': all_classes,                                                             # ASTì—ì„œ ì¶”ì¶œí•œ í´ë˜ìŠ¤ ì •ë³´
            'methods': all_methods,                                                             # ASTì—ì„œ ì¶”ì¶œí•œ ë©”ì†Œë“œ ì •ë³´
            'fields': all_fields,                                                               # ASTì—ì„œ ì¶”ì¶œí•œ í•„ë“œ ì •ë³´
            'relationships': relationships,                                                     # ëª¨ë“  ë ˆë²¨ì˜ ê´€ê³„ ì •ë³´
            'cycles': all_cycles,                                                               # ëª¨ë“  ë ˆë²¨ì˜ ìˆœí™˜ ì°¸ì¡°
            'metrics': enhanced_metrics                                                         # ê³„ì‚°ëœ í’ˆì§ˆ ë©”íŠ¸ë¦­
        }
    
    def _detect_detailed_cycles(self, classes: List[ClassInfo], methods: List[MethodInfo],
                              relationships: List[Relationship]) -> List[Dict]:
        """í´ë˜ìŠ¤ì™€ ë©”ì†Œë“œ ë ˆë²¨ì˜ ìƒì„¸í•œ ìˆœí™˜ ì°¸ì¡° íƒì§€"""
        cycles = []                                                                             # íƒì§€ëœ ìˆœí™˜ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸

        # ê´€ê³„ë“¤ë¡œë¶€í„° ì¸ì ‘ ê·¸ë˜í”„ êµ¬ì¶• (ë°©í–¥ì„± ê·¸ë˜í”„)                                           # ì˜ì¡´ì„± ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„
        graph = {}                                                                              # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ê·¸ë˜í”„
        for rel in relationships:                                                               # ëª¨ë“  ê´€ê³„ì— ëŒ€í•´
            if rel.from_entity not in graph:                                                   # ì†ŒìŠ¤ ì—”í‹°í‹°ê°€ ê·¸ë˜í”„ì— ì—†ìœ¼ë©´
                graph[rel.from_entity] = set()                                                 # ë¹ˆ ì§‘í•©ìœ¼ë¡œ ì´ˆê¸°í™”
            graph[rel.from_entity].add(rel.to_entity)                                         # íƒ€ê²Ÿ ì—”í‹°í‹° ì¶”ê°€ (ë°©í–¥ì„± ê°„ì„ )

        # ë‹¨ìˆœí•œ DFS ê¸°ë°˜ ìˆœí™˜ íƒì§€ (ê° ë…¸ë“œì—ì„œ ì‹œì‘)                                          # ê¹Šì´ ìš°ì„  íƒìƒ‰ìœ¼ë¡œ ìˆœí™˜ ì°¾ê¸°
        visited = set()                                                                        # ì „ì²´ íƒìƒ‰ì—ì„œ ë°©ë¬¸í•œ ë…¸ë“œë“¤

        def has_cycle(node, path):
            """í˜„ì¬ ê²½ë¡œì—ì„œ ìˆœí™˜ì´ ìˆëŠ”ì§€ í™•ì¸"""
            if node in path:                                                                   # í˜„ì¬ ê²½ë¡œì— ì´ë¯¸ ìˆìœ¼ë©´ ìˆœí™˜ ë°œê²¬
                cycle_start = path.index(node)                                                 # ìˆœí™˜ ì‹œì‘ ì§€ì  ì°¾ê¸°
                return path[cycle_start:] + [node]                                             # ìˆœí™˜ ê²½ë¡œ ë°˜í™˜

            if node not in graph:                                                              # ë” ì´ìƒ ì—°ê²°ëœ ë…¸ë“œê°€ ì—†ìœ¼ë©´
                return None                                                                    # ìˆœí™˜ ì—†ìŒ

            path.append(node)                                                                  # í˜„ì¬ ë…¸ë“œë¥¼ ê²½ë¡œì— ì¶”ê°€
            for neighbor in graph[node]:                                                       # ì—°ê²°ëœ ëª¨ë“  ì´ì›ƒ ë…¸ë“œì— ëŒ€í•´
                cycle = has_cycle(neighbor, path)                                              # ì¬ê·€ì ìœ¼ë¡œ ìˆœí™˜ íƒì§€
                if cycle:                                                                      # ìˆœí™˜ì´ ë°œê²¬ë˜ë©´
                    return cycle                                                               # ìˆœí™˜ ê²½ë¡œ ë°˜í™˜
            path.pop()                                                                         # ë°±íŠ¸ë˜í‚¹ (í˜„ì¬ ê²½ë¡œì—ì„œ ë…¸ë“œ ì œê±°)
            return None                                                                        # ì´ ê²½ë¡œì—ì„œëŠ” ìˆœí™˜ ì—†ìŒ

        # ëª¨ë“  ë…¸ë“œì—ì„œ ìˆœí™˜ íƒì§€ ì‹œì‘                                                           # ì—°ê²°ë˜ì§€ ì•Šì€ ì»´í¬ë„ŒíŠ¸ë„ ëª¨ë‘ í™•ì¸
        for node in graph:                                                                     # ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œì— ëŒ€í•´
            if node not in visited:                                                            # ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë…¸ë“œë©´
                cycle = has_cycle(node, [])                                                     # ìˆœí™˜ íƒì§€ ì‹œì‘
                if cycle:                                                                       # ìˆœí™˜ì´ ë°œê²¬ë˜ë©´
                    cycle_info = {                                                              # ìˆœí™˜ ì •ë³´ ìƒì„±
                        'id': f"detailed_cycle_{len(cycles)}",                                 # ê³ ìœ  ìˆœí™˜ ID
                        'entities': cycle,                                                      # ìˆœí™˜ì— ì°¸ì—¬í•˜ëŠ” ì—”í‹°í‹°ë“¤
                        'cycle_type': 'call',  # ëŒ€ë¶€ë¶„ì˜ ìƒì„¸ ìˆœí™˜ì€ ë©”ì†Œë“œ í˜¸ì¶œ               # ìˆœí™˜ íƒ€ì…
                        'severity': 'low' if len(cycle) <= 2 else 'medium',                   # ì‹¬ê°ë„ (ê¸¸ì´ì— ë”°ë¼)
                        'description': f"Call cycle involving {len(cycle)} entities"           # ìˆœí™˜ ì„¤ëª…
                    }
                    cycles.append(cycle_info)                                                   # ìˆœí™˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        return cycles                                                                           # íƒì§€ëœ ëª¨ë“  ìˆœí™˜ ì°¸ì¡° ë°˜í™˜

    def _detect_import_cycles_from_modules(self, modules: List[ModuleInfo]) -> List[Dict]:
        """Detect import cycles using consolidated ModuleInfo.imports.
        This complements AST-based detection and helps catch cycles missed by path-based normalization."""
        cycles: List[Dict] = []
        if not modules:
            return cycles

        # Build set of project modules (exclude external libraries)
        project_modules = {m.name for m in modules}

        if DEBUG_MODE:
            with open('/tmp/pyview_debug.log', 'a') as f:
                f.write(f"ğŸ” CYCLE DEBUG: Project modules: {list(project_modules)[:10]}...\n")

        # Build module import graph: module_name -> set(imported_module_name)
        # Only include imports between project modules to avoid false positives with external libraries
        graph: Dict[str, Set[str]] = {}
        external_imports = set()

        for m in modules:
            src = m.name
            graph.setdefault(src, set())
            for imp in getattr(m, 'imports', []) or []:
                target = imp.module
                if target:
                    if target in project_modules:
                        # Only add to graph if target is also a project module (not external library)
                        graph[src].add(target)
                    else:
                        external_imports.add(target)

        if DEBUG_MODE:
            with open('/tmp/pyview_debug.log', 'a') as f:
                f.write(f"ğŸ” CYCLE DEBUG: External imports found: {list(external_imports)[:10]}...\n")
                f.write(f"ğŸ” CYCLE DEBUG: Internal graph edges: {sum(len(edges) for edges in graph.values())}\n")

        # Kosaraju to find SCCs
        visited: Set[str] = set()
        order: List[str] = []
        def dfs1(n: str) -> None:
            visited.add(n)
            for nb in graph.get(n, set()):
                if nb not in visited and nb in graph:
                    dfs1(nb)
            order.append(n)
        for n in list(graph.keys()):
            if n not in visited:
                dfs1(n)
        transpose: Dict[str, Set[str]] = {}
        for u, nbrs in graph.items():
            transpose.setdefault(u, set())
            for v in nbrs:
                transpose.setdefault(v, set()).add(u)
        visited.clear()
        def dfs2(n: str, comp: List[str]) -> None:
            visited.add(n)
            comp.append(n)
            for nb in transpose.get(n, set()):
                if nb not in visited:
                    dfs2(nb, comp)
        cycle_id = 0
        for n in reversed(order):
            if n not in visited:
                comp: List[str] = []
                dfs2(n, comp)
                if len(comp) >= 2:
                    paths = []
                    for u in comp:
                        for v in graph.get(u, set()):
                            if v in comp:
                                paths.append({
                                    'from': create_module_id(u),
                                    'to': create_module_id(v),
                                    'relationship_type': 'import',
                                    'strength': 1.0
                                })

                    cycle_info = {
                        'id': f"mod_import_cycle_{cycle_id}",
                        'entities': [create_module_id(x) for x in comp],
                        'paths': paths,
                        'cycle_type': 'import',
                        'severity': 'high' if len(comp) > 3 else 'medium',
                        'description': f"Module import cycle involving {len(comp)} modules",
                        'metrics': {
                            'length': len(comp),
                            'detection_method': 'module_list'
                        }
                    }

                    if DEBUG_MODE:
                        with open('/tmp/pyview_debug.log', 'a') as f:
                            f.write(f"ğŸ” CYCLE FOUND: {cycle_info['description']}\n")
                            f.write(f"ğŸ” CYCLE ENTITIES: {comp}\n")
                            f.write(f"ğŸ” CYCLE PATHS: {len(paths)} edges\n")

                    cycles.append(cycle_info)
                    cycle_id += 1
        return cycles
    
    def _detect_cycles_by_type(self, relationships: List[Relationship], cycle_type: str) -> List[Dict]:
        """Detect cycles for a specific relationship type"""
        cycles = []
        
        if not relationships:
            return cycles
        
        # Build adjacency graph
        graph = {}
        edge_info = {}  # Store relationship details
        
        for rel in relationships:
            if rel.from_entity not in graph:
                graph[rel.from_entity] = set()
            graph[rel.from_entity].add(rel.to_entity)
            edge_info[(rel.from_entity, rel.to_entity)] = rel
        
        # Find strongly connected components using DFS
        visited = set()
        finished = set()
        stack = []
        
        def dfs1(node):
            if node in visited:
                return
            visited.add(node)
            for neighbor in graph.get(node, []):
                dfs1(neighbor)
            stack.append(node)
        
        # First DFS to get finish times
        for node in graph:
            dfs1(node)
        
        # Build reverse graph
        reverse_graph = {}
        for node in graph:
            for neighbor in graph[node]:
                if neighbor not in reverse_graph:
                    reverse_graph[neighbor] = set()
                reverse_graph[neighbor].add(node)
        
        # Second DFS on reverse graph
        visited.clear()
        
        def dfs2(node, component):
            if node in visited:
                return
            visited.add(node)
            component.append(node)
            for neighbor in reverse_graph.get(node, []):
                dfs2(neighbor, component)
        
        # Find SCCs
        while stack:
            node = stack.pop()
            if node not in visited:
                component = []
                dfs2(node, component)
                
                # Only consider components with cycles (size > 1)
                if len(component) > 1:
                    # Extract cycle path
                    cycle_paths = []
                    for i, entity in enumerate(component):
                        next_entity = component[(i + 1) % len(component)]
                        # Check if direct edge exists
                        if entity in graph and next_entity in graph[entity]:
                            rel = edge_info.get((entity, next_entity))
                            if rel:
                                cycle_paths.append({
                                    'from': entity,
                                    'to': next_entity,
                                    'relationship_type': cycle_type,
                                    'strength': rel.strength if hasattr(rel, 'strength') else 1.0,
                                    'line_number': rel.line_number,
                                    'file_path': rel.file_path
                                })
                    
                    # Calculate severity based on cycle type and length
                    if cycle_type == 'import':
                        severity = 'high' if len(component) > 3 else 'medium'
                    else:
                        severity = 'low' if len(component) <= 2 else 'medium'
                    
                    cycle_info = {
                        'id': f"{cycle_type}_cycle_{len(cycles)}",
                        'entities': component,
                        'paths': cycle_paths,
                        'cycle_type': cycle_type,
                        'severity': severity,
                        'metrics': {
                            'length': len(component),
                            'edge_count': len(cycle_paths)
                        },
                        'description': f"{cycle_type.title()} cycle involving {len(component)} entities"
                    }
                    cycles.append(cycle_info)

        return cycles
    
    def _detect_import_cycles_from_ast(self, ast_analyses: List[FileAnalysis]) -> List[Dict]:
        """Detect import cycles from AST analysis when pydeps fails"""
        cycles = []
        
        if not ast_analyses:
            return cycles
        
        # Build import graph from AST analysis
        import_graph: Dict[str, Set[str]] = {}
        file_to_module: Dict[str, str] = {}
        project_modules: Set[str] = set()

        # First pass: collect all project modules
        for analysis in ast_analyses:
            if not analysis or not analysis.file_path:
                continue
            module_name = self._file_path_to_module_name(analysis.file_path)
            project_modules.add(module_name)
            file_to_module[analysis.file_path] = module_name

        if DEBUG_MODE:
            with open('/tmp/pyview_debug.log', 'a') as f:
                f.write(f"ğŸ” AST DEBUG: Found {len(project_modules)} project modules\n")
                f.write(f"ğŸ” AST DEBUG: Project modules: {list(project_modules)[:10]}...\n")

        # Second pass: build graph with only project-internal imports
        for analysis in ast_analyses:
            if not analysis or not analysis.file_path:
                continue

            module_name = self._file_path_to_module_name(analysis.file_path)
            import_graph.setdefault(module_name, set())

            # Extract imports from AST analysis
            for import_info in analysis.imports:
                # Convert relative imports to absolute module names (best-effort)
                imported_module = self._resolve_import_name(
                    import_info.module, analysis.file_path
                )
                # Only add to graph if target is also a project module (not external library)
                if imported_module and imported_module in project_modules:
                    import_graph[module_name].add(imported_module)

        # Use Kosaraju's algorithm to find all strongly connected components
        # Step 1: Order vertices by finish time
        visited: Set[str] = set()
        order: List[str] = []

        def dfs1(node: str) -> None:
            visited.add(node)
            for neighbor in import_graph.get(node, set()):
                if neighbor not in visited and neighbor in import_graph:
                    dfs1(neighbor)
            order.append(node)

        for node in list(import_graph.keys()):
            if node not in visited:
                dfs1(node)

        # Step 2: Transpose graph
        transpose: Dict[str, Set[str]] = {}
        for u, neighbors in import_graph.items():
            transpose.setdefault(u, set())
            for v in neighbors:
                transpose.setdefault(v, set()).add(u)

        # Step 3: DFS on transposed graph to get SCCs
        visited.clear()

        def dfs2(node: str, component: List[str]) -> None:
            visited.add(node)
            component.append(node)
            for neighbor in transpose.get(node, set()):
                if neighbor not in visited:
                    dfs2(neighbor, component)

        cycle_id = 0
        for node in reversed(order):
            if node not in visited:
                component: List[str] = []
                dfs2(node, component)

                # Emit cycles for SCCs with size >= 2
                if len(component) >= 2:
                    cycle_paths: List[Dict] = []
                    # Add edges within the component as cycle paths
                    for u in component:
                        for v in import_graph.get(u, set()):
                            if v in component:
                                cycle_paths.append({
                                    'from': create_module_id(u),
                                    'to': create_module_id(v),
                                    'relationship_type': 'import',
                                    'strength': 1.0
                                })
                    cycle_info = {
                        'id': f"ast_import_cycle_{cycle_id}",
                        'entities': [create_module_id(x) for x in component],
                        'paths': cycle_paths,
                        'cycle_type': 'import',
                        'severity': 'high' if len(component) > 3 else 'medium',
                        'description': f"AST-detected import cycle involving {len(component)} modules",
                        'metrics': {
                            'length': len(component),
                            'detection_method': 'ast'
                        }
                    }

                    if DEBUG_MODE:
                        with open('/tmp/pyview_debug.log', 'a') as f:
                            f.write(f"ğŸ” AST CYCLE FOUND: {cycle_info['description']}\n")
                            f.write(f"ğŸ” AST CYCLE ENTITIES: {component}\n")
                            f.write(f"ğŸ” AST CYCLE PATHS: {len(cycle_paths)} edges\n")

                    cycles.append(cycle_info)
                    cycle_id += 1
                # Handle self-loop (module importing itself)
                elif len(component) == 1:
                    u = component[0]
                    # Only flag as a cycle if:
                    # 1. The module actually imports itself (self-reference)
                    # 2. The module has actual imports (not just an empty set)
                    u_imports = import_graph.get(u, set())

                    if u in u_imports and len(u_imports) > 0:
                        cycles.append({
                            'id': f"ast_import_cycle_{cycle_id}",
                            'entities': [create_module_id(u)],
                            'paths': [{
                                'from': create_module_id(u),
                                'to': create_module_id(u),
                                'relationship_type': 'import',
                                'strength': 1.0
                            }],
                            'cycle_type': 'import',
                            'severity': 'medium',
                            'description': "AST-detected self import cycle",
                            'metrics': {
                                'length': 1,
                                'detection_method': 'ast'
                            }
                        })
                        cycle_id += 1

        return cycles
    
    def _file_path_to_module_name(self, file_path: str) -> str:
        """Convert file path to module name"""
        # Remove .py extension
        if file_path.endswith('.py'):
            module_path = file_path[:-3]
        else:
            module_path = file_path

        # Convert path separators to dots and create a proper module path
        import os

        # Split the path and filter out non-meaningful parts
        parts = module_path.replace(os.sep, '/').split('/')

        # Find the start of the actual module path (skip system paths)
        start_idx = 0
        for i, part in enumerate(parts):
            # Look for common Python package indicators
            if part in ['src', 'lib', 'autorag'] or part.endswith('.py') or (i > 0 and '.' in part):
                start_idx = i
                break

        # Build module name from meaningful parts
        meaningful_parts = parts[start_idx:]
        if meaningful_parts:
            # Remove empty parts and join with dots
            module_name = '.'.join(part for part in meaningful_parts if part and part != '__pycache__')
        else:
            # Fallback to basename if no meaningful parts found
            module_name = os.path.basename(module_path)

        return module_name
    
    def _resolve_import_name(self, import_name: str, current_file: str) -> Optional[str]:
        """Resolve import name to absolute module name"""
        # For now, return the import name as is
        # In a full implementation, this would handle relative imports properly
        if import_name.startswith('.'):
            # Relative import - for now, just strip the dot
            return import_name.lstrip('.')
        return import_name
    
    def _calculate_enhanced_metrics(self, packages: List[PackageInfo], modules: List[ModuleInfo],
                                  classes: List[ClassInfo], methods: List[MethodInfo],
                                  relationships: List[Relationship]) -> Dict:
        """5ë‹¨ê³„ ëª¨ë“  ë ˆë²¨ì„ í¬í•¨í•œ í–¥ìƒëœ ë©”íŠ¸ë¦­ ê³„ì‚°"""

        metrics = {
            'entity_counts': {                                                              # ì—”í‹°í‹° ê°œìˆ˜ í†µê³„
                'packages': len(packages),                                                  # íŒ¨í‚¤ì§€ ê°œìˆ˜
                'modules': len(modules),                                                    # ëª¨ë“ˆ ê°œìˆ˜
                'classes': len(classes),                                                    # í´ë˜ìŠ¤ ê°œìˆ˜
                'methods': len(methods),                                                    # ë©”ì†Œë“œ ê°œìˆ˜
                'relationships': len(relationships)                                        # ê´€ê³„ ê°œìˆ˜
            },
            'complexity_metrics': {},                                                       # ë³µì¡ë„ ë©”íŠ¸ë¦­ë“¤
            'coupling_metrics': {},                                                         # ê²°í•©ë„ ë©”íŠ¸ë¦­ë“¤
            'quality_metrics': {}                                                           # í’ˆì§ˆ ë©”íŠ¸ë¦­ë“¤
        }
        # ë³µì¡ë„ ë©”íŠ¸ë¦­ ê³„ì‚°                                                                     # ê° ë©”ì†Œë“œì˜ ìˆœí™˜ ë³µì¡ë„ ìˆ˜ì§‘
        for method in methods:                                                                  # ëª¨ë“  ë©”ì†Œë“œì— ëŒ€í•´
            if method.complexity:                                                               # ë³µì¡ë„ ì •ë³´ê°€ ìˆìœ¼ë©´
                metrics['complexity_metrics'][method.id] = method.complexity                   # ë©”ì†Œë“œ IDì™€ ë³µì¡ë„ ë§¤í•‘

        # ê²°í•©ë„ ë©”íŠ¸ë¦­ ê³„ì‚°                                                                     # ì—”í‹°í‹° ê°„ ì˜ì¡´ì„± ê°•ë„ ì¸¡ì •
        in_degree = {}                                                                          # ë“¤ì–´ì˜¤ëŠ” ì˜ì¡´ì„± ê°œìˆ˜ (afferent coupling)
        out_degree = {}                                                                         # ë‚˜ê°€ëŠ” ì˜ì¡´ì„± ê°œìˆ˜ (efferent coupling)

        for rel in relationships:                                                               # ëª¨ë“  ê´€ê³„ì— ëŒ€í•´
            # ë“¤ì–´ì˜¤ëŠ” ê´€ê³„ ì¹´ìš´íŠ¸ (ë‹¤ë¥¸ ì—”í‹°í‹°ê°€ ì´ ì—”í‹°í‹°ì— ì˜ì¡´)                             # í•´ë‹¹ ì—”í‹°í‹°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ì—”í‹°í‹° ìˆ˜
            if rel.to_entity not in in_degree:                                                 # íƒ€ê²Ÿ ì—”í‹°í‹°ê°€ ë”•ì…”ë„ˆë¦¬ì— ì—†ìœ¼ë©´
                in_degree[rel.to_entity] = 0                                                   # 0ìœ¼ë¡œ ì´ˆê¸°í™”
            in_degree[rel.to_entity] += 1                                                      # ë“¤ì–´ì˜¤ëŠ” ì˜ì¡´ì„± ì¹´ìš´íŠ¸ ì¦ê°€

            # ë‚˜ê°€ëŠ” ê´€ê³„ ì¹´ìš´íŠ¸ (ì´ ì—”í‹°í‹°ê°€ ë‹¤ë¥¸ ì—”í‹°í‹°ì— ì˜ì¡´)                               # í•´ë‹¹ ì—”í‹°í‹°ê°€ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ì—”í‹°í‹° ìˆ˜
            if rel.from_entity not in out_degree:                                              # ì†ŒìŠ¤ ì—”í‹°í‹°ê°€ ë”•ì…”ë„ˆë¦¬ì— ì—†ìœ¼ë©´
                out_degree[rel.from_entity] = 0                                                # 0ìœ¼ë¡œ ì´ˆê¸°í™”
            out_degree[rel.from_entity] += 1                                                   # ë‚˜ê°€ëŠ” ì˜ì¡´ì„± ì¹´ìš´íŠ¸ ì¦ê°€

        # ê° ì—”í‹°í‹°ì˜ ë¶ˆì•ˆì •ì„± ê³„ì‚° (instability = Ce / (Ca + Ce))                            # ë¶ˆì•ˆì •ì„±ì€ ë³€ê²½ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ë‚˜íƒ€ëƒ„
        all_entities = set(in_degree.keys()) | set(out_degree.keys())                         # ëª¨ë“  ì—”í‹°í‹° ì§‘í•©
        for entity in all_entities:                                                            # ê° ì—”í‹°í‹°ì— ëŒ€í•´
            ca = in_degree.get(entity, 0)  # Afferent coupling (ë“¤ì–´ì˜¤ëŠ” ì˜ì¡´ì„±)                # ì´ ì—”í‹°í‹°ì— ì˜ì¡´í•˜ëŠ” ë‹¤ë¥¸ ì—”í‹°í‹° ìˆ˜
            ce = out_degree.get(entity, 0)  # Efferent coupling (ë‚˜ê°€ëŠ” ì˜ì¡´ì„±)                # ì´ ì—”í‹°í‹°ê°€ ì˜ì¡´í•˜ëŠ” ë‹¤ë¥¸ ì—”í‹°í‹° ìˆ˜

            instability = ce / (ca + ce) if (ca + ce) > 0 else 0.0                           # ë¶ˆì•ˆì •ì„± ì§€ìˆ˜ ê³„ì‚° (0~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶ˆì•ˆì •)
            metrics['coupling_metrics'][entity] = {                                            # ì—”í‹°í‹°ë³„ ê²°í•©ë„ ë©”íŠ¸ë¦­ ì €ì¥
                'afferent_coupling': ca,                                                       # ë“¤ì–´ì˜¤ëŠ” ê²°í•©ë„
                'efferent_coupling': ce,                                                       # ë‚˜ê°€ëŠ” ê²°í•©ë„
                'instability': instability                                                     # ë¶ˆì•ˆì •ì„± ì§€ìˆ˜
            }
        return metrics                                                                          # ê³„ì‚°ëœ ëª¨ë“  ë©”íŠ¸ë¦­ ë°˜í™˜
    
    def _calculate_quality_metrics(self, integrated_data: Dict, project_files: List[str],
                                  progress_callback: ProgressCallback) -> List[QualityMetrics]:
        """ëª¨ë“  ì—”í‹°í‹°ì— ëŒ€í•œ ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        quality_metrics = []                                                                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        if not self.metrics_engine:                                                             # ë©”íŠ¸ë¦­ ì—”ì§„ì´ ì—†ìœ¼ë©´
            return quality_metrics                                                              # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            
        from .models import EntityType                                                          # EntityType ì„í¬íŠ¸

        # ë¶„ì„ì„ ìœ„í•œ ì†ŒìŠ¤ íŒŒì¼ ì½ê¸°                                                             # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ìœ„í•´ ì›ë³¸ ì†ŒìŠ¤ ì½”ë“œ í•„ìš”
        source_cache = {}                                                                       # íŒŒì¼ ê²½ë¡œë³„ ì†ŒìŠ¤ ì½”ë“œ ìºì‹œ
        for file_path in project_files:                                                        # ëª¨ë“  í”„ë¡œì íŠ¸ íŒŒì¼ì— ëŒ€í•´
            try:
                with open(file_path, 'r', encoding='utf-8') as f:                              # UTF-8ë¡œ íŒŒì¼ ì—´ê¸°
                    source_cache[file_path] = f.read()                                         # íŒŒì¼ ë‚´ìš©ì„ ìºì‹œì— ì €ì¥
            except:                                                                             # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ì‹œ
                continue                                                                        # í•´ë‹¹ íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ê³„ì†

        total_entities = (len(integrated_data.get('modules', [])) +                            # ì´ ì—”í‹°í‹° ìˆ˜ ê³„ì‚° (ì§„í–‰ë¥  í‘œì‹œìš©)
                         len(integrated_data.get('classes', [])))                             # ëª¨ë“ˆê³¼ í´ë˜ìŠ¤ ìˆ˜ì˜ í•©
        processed = 0                                                                           # ì²˜ë¦¬ëœ ì—”í‹°í‹° ìˆ˜

        # ëª¨ë“ˆì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°                                                               # ëª¨ë“ˆ ë ˆë²¨ í’ˆì§ˆ ë¶„ì„
        for module in integrated_data.get('modules', []):                                      # ëª¨ë“  ëª¨ë“ˆì— ëŒ€í•´
            # ê¸°ì¡´ì—ëŠ” ClassInfoë¥¼ ê°ì²´ë¡œë§Œ ë°›ì•˜ìœ¼ë‚˜, ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ ë„˜ê²¨ì£¼ëŠ” ë ˆê±°ì‹œ ì½”ë“œê°€ ìˆì–´ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
            file_path = module.file_path if hasattr(module, 'file_path') else module.get('file_path', '')
            entity_id = module.id if hasattr(module, 'id') else module.get('id', 'unknown')

            source_code = source_cache.get(file_path, "")                                     # í•´ë‹¹ ëª¨ë“ˆì˜ ì†ŒìŠ¤ ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
            if source_code:                                                                     # ì†ŒìŠ¤ ì½”ë“œê°€ ìˆìœ¼ë©´
                module_metrics = self.metrics_engine.analyze_module_quality(module, source_code)  # ëª¨ë“ˆ í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰

                quality_metric = QualityMetrics(                                               # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
                    entity_id=entity_id,                                                       # ëª¨ë“ˆ ID
                    entity_type=EntityType.MODULE,                                            # ì—”í‹°í‹° íƒ€ì… (ëª¨ë“ˆ)
                    cyclomatic_complexity=module_metrics.complexity.cyclomatic_complexity,    # ìˆœí™˜ ë³µì¡ë„
                    cognitive_complexity=module_metrics.complexity.cognitive_complexity,      # ì¸ì§€ ë³µì¡ë„
                    nesting_depth=module_metrics.complexity.nesting_depth,                    # ì¤‘ì²© ê¹Šì´
                    lines_of_code=module_metrics.complexity.lines_of_code,                    # ì½”ë“œ ë¼ì¸ ìˆ˜
                    maintainability_index=module_metrics.maintainability_index,               # ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜
                    quality_grade=self.metrics_engine.get_quality_rating(module_metrics)      # í’ˆì§ˆ ë“±ê¸‰
                )
                quality_metrics.append(quality_metric)                                        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

            processed += 1                                                                     # ì²˜ë¦¬ëœ ì—”í‹°í‹° ìˆ˜ ì¦ê°€
            if processed % 10 == 0:                                                            # 10ê°œë§ˆë‹¤ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = 85 + (processed / total_entities) * 10                             # ì§„í–‰ë¥  ê³„ì‚° (85%~95%)
                progress_callback.update(f"Quality metrics: {processed}/{total_entities}", progress)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        # í´ë˜ìŠ¤ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°                                                             # í´ë˜ìŠ¤ ë ˆë²¨ í’ˆì§ˆ ë¶„ì„
        for class_info in integrated_data.get('classes', []):                                  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´
            # ê¸°ì¡´ì—ëŠ” ClassInfoë¥¼ ê°ì²´ë¡œë§Œ ë°›ì•˜ìœ¼ë‚˜, ë”•ì…”ë„ˆë¦¬ë¡œ ë§Œë“¤ì–´ ë„˜ê²¨ì£¼ëŠ” ë ˆê±°ì‹œ ì½”ë“œê°€ ìˆì–´ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
            file_path = class_info.file_path if hasattr(class_info, 'file_path') else class_info.get('file_path', '')
            entity_id = class_info.id if hasattr(class_info, 'id') else class_info.get('id', 'unknown')

            source_code = source_cache.get(file_path, "")                                     # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì†ŒìŠ¤ ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
            if source_code:                                                                     # ì†ŒìŠ¤ ì½”ë“œê°€ ìˆìœ¼ë©´
                class_metrics = self.metrics_engine.analyze_class_quality(class_info, source_code)  # í´ë˜ìŠ¤ í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰

                quality_metric = QualityMetrics(                                               # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê°ì²´ ìƒì„±
                    entity_id=entity_id,                                                       # í´ë˜ìŠ¤ ID
                    entity_type=EntityType.CLASS,                                             # ì—”í‹°í‹° íƒ€ì… (í´ë˜ìŠ¤)
                    cyclomatic_complexity=class_metrics.complexity.cyclomatic_complexity,     # ìˆœí™˜ ë³µì¡ë„
                    cognitive_complexity=class_metrics.complexity.cognitive_complexity,       # ì¸ì§€ ë³µì¡ë„
                    nesting_depth=class_metrics.complexity.nesting_depth,                     # ì¤‘ì²© ê¹Šì´
                    lines_of_code=class_metrics.complexity.lines_of_code,                     # ì½”ë“œ ë¼ì¸ ìˆ˜
                    maintainability_index=class_metrics.maintainability_index,                # ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜
                    quality_grade=self.metrics_engine.get_quality_rating(class_metrics)       # í’ˆì§ˆ ë“±ê¸‰
                )
                quality_metrics.append(quality_metric)                                        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

            processed += 1                                                                     # ì²˜ë¦¬ëœ ì—”í‹°í‹° ìˆ˜ ì¦ê°€
            if processed % 10 == 0:                                                            # 10ê°œë§ˆë‹¤ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = 85 + (processed / total_entities) * 10                             # ì§„í–‰ë¥  ê³„ì‚° (85%~95%)
                progress_callback.update(f"Quality metrics: {processed}/{total_entities}", progress)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        self.logger.info(f"Calculated quality metrics for {len(quality_metrics)} entities")  # ê³„ì‚°ëœ ë©”íŠ¸ë¦­ ìˆ˜ ë¡œê·¸ ì¶œë ¥
        return quality_metrics                                                                  # ê³„ì‚°ëœ ëª¨ë“  í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°˜í™˜
    
    def _assemble_result(self, project_path: str, integrated_data: Dict,
                        quality_metrics: List[QualityMetrics],
                        start_time: float, progress_callback: ProgressCallback) -> AnalysisResult:
        """ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°ë¦½í•˜ì—¬ AnalysisResult ê°ì²´ ìƒì„±"""
        # ë¶„ì„ ì™„ë£Œ ì‹œê°„ ê³„ì‚° ë° í”„ë¡œì íŠ¸ ì •ë³´ ìƒì„±

        end_time = time.time()                                # ë¶„ì„ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        duration = end_time - start_time                     # ì´ ë¶„ì„ ì†Œìš” ì‹œê°„ ê³„ì‚°

        # í”„ë¡œì íŠ¸ ì •ë³´ ê°ì²´ ìƒì„±
        project_info = ProjectInfo(
            name=os.path.basename(project_path),             # í”„ë¡œì íŠ¸ ì´ë¦„ (í´ë”ëª…)
            path=project_path,                                # í”„ë¡œì íŠ¸ ì „ì²´ ê²½ë¡œ
            analyzed_at=datetime.now().isoformat(),          # ë¶„ì„ ì™„ë£Œ ì‹œê° (ISO í˜•ì‹)
            total_files=self.total_files,                    # ë¶„ì„ëœ ì´ íŒŒì¼ ìˆ˜
            analysis_duration_seconds=duration,              # ë¶„ì„ ì†Œìš” ì‹œê°„ (ì´ˆ)
            python_version=f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",  # Python ë²„ì „ ì •ë³´
            analysis_options=vars(self.options)              # ë¶„ì„ ì˜µì…˜ ì„¤ì •ê°’ë“¤
        )

        # ì˜ì¡´ì„± ê·¸ë˜í”„ ê°ì²´ ìƒì„± (5ê³„ì¸µ êµ¬ì¡°)
        dependency_graph = DependencyGraph(
            packages=integrated_data['packages'],            # íŒ¨í‚¤ì§€ ê³„ì¸µ
            modules=integrated_data['modules'],              # ëª¨ë“ˆ ê³„ì¸µ
            classes=integrated_data['classes'],              # í´ë˜ìŠ¤ ê³„ì¸µ
            methods=integrated_data['methods'],              # ë©”ì„œë“œ ê³„ì¸µ
            fields=integrated_data['fields']                 # í•„ë“œ ê³„ì¸µ
        )

        # ìˆœí™˜ ì˜ì¡´ì„± ë”•ì…”ë„ˆë¦¬ë¥¼ CyclicDependency ê°ì²´ë¡œ ë³€í™˜
        cycles = []                                           # ìˆœí™˜ ì˜ì¡´ì„± ê°ì²´ ëª©ë¡
        for cycle_dict in integrated_data['cycles']:         # ê° ìˆœí™˜ ì˜ì¡´ì„±ì— ëŒ€í•´
            cycle = CyclicDependency(
                id=cycle_dict['id'],                         # ìˆœí™˜ ì˜ì¡´ì„± ê³ ìœ  ID
                entities=cycle_dict['entities'],             # ìˆœí™˜ì— í¬í•¨ëœ ì—”í‹°í‹°ë“¤
                cycle_type=cycle_dict['cycle_type'],         # ìˆœí™˜ íƒ€ì… (module/class/method)
                severity=cycle_dict['severity'],             # ì‹¬ê°ë„ ìˆ˜ì¤€
                description=cycle_dict.get('description')    # ìˆœí™˜ ì˜ì¡´ì„± ì„¤ëª…
            )
            cycles.append(cycle)                              # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        # ìµœì¢… ë¶„ì„ ê²°ê³¼ ê°ì²´ ìƒì„±
        result = AnalysisResult(
            analysis_id=self.current_analysis_id,            # ê³ ìœ  ë¶„ì„ ID
            project_info=project_info,                       # í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
            dependency_graph=dependency_graph,               # 5ê³„ì¸µ ì˜ì¡´ì„± ê·¸ë˜í”„
            relationships=integrated_data['relationships'],  # ê´€ê³„ ì •ë³´
            quality_metrics=quality_metrics,                 # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê²°ê³¼
            metrics=integrated_data['metrics'],              # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì •ë³´
            cycles=cycles                                     # ìˆœí™˜ ì˜ì¡´ì„± ëª©ë¡
        )

        # ë¶„ì„ ì™„ë£Œ ë¡œê·¸ ì¶œë ¥
        self.logger.info(f"Analysis completed in {duration:.2f} seconds")    # ë¶„ì„ ì†Œìš” ì‹œê°„
        self.logger.info(f"Found {len(result.dependency_graph.modules)} modules, "  # ë°œê²¬ëœ êµ¬ì„±ìš”ì†Œ ìˆ˜
                        f"{len(result.dependency_graph.classes)} classes, "
                        f"{len(result.dependency_graph.methods)} methods")

        return result                                         # ì™„ì„±ëœ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    # === ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ê²½ë¡œ (>= 1000 íŒŒì¼) ===

    def _analyze_large_project(self, project_path: str, project_files: List[str],
                              progress_callback: ProgressCallback, start_time: float) -> AnalysisResult:
        """ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸(>=1000íŒŒì¼)ì— ìµœì í™”ëœ ë¶„ì„ ì „ëµ ì‚¬ìš©"""
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì²˜ë¦¬ ì†ë„ í–¥ìƒì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„

        if not self.large_project_analyzer:                   # ëŒ€ê·œëª¨ ë¶„ì„ê¸°ê°€ ì—†ëŠ” ê²½ìš°
            # í‘œì¤€ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´
            return self._perform_full_analysis(project_path, project_files, progress_callback, start_time)

        progress_callback.update("Initializing large project analysis", 12)  # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ì´ˆê¸°í™”

        # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìµœì í™”ëœ ë¶„ì„ í•¨ìˆ˜ ìƒì„±
        def optimized_ast_analysis(file_batch: List[str]):    # ë°°ì¹˜ ë‹¨ìœ„ AST ë¶„ì„ í•¨ìˆ˜
            batch_results = []                                # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì €ì¥
            for file_path in file_batch:                      # ë°°ì¹˜ ë‚´ ê° íŒŒì¼ì— ëŒ€í•´
                try:
                    analysis = self.ast_analyzer.analyze_file(file_path)  # AST ë¶„ì„ ìˆ˜í–‰
                    if analysis:                              # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´
                        batch_results.append(analysis)       # ê²°ê³¼ì— ì¶”ê°€
                except Exception as e:                        # ë¶„ì„ ì‹¤íŒ¨ ì‹œ
                    self.logger.warning(f"Failed to analyze {file_path}: {e}")  # ê²½ê³  ë¡œê·¸
            return batch_results                              # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

        # ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        all_analyses = []                                     # ì „ì²´ ë¶„ì„ ê²°ê³¼ ëˆ„ì 
        total_processed = 0                                   # ì²˜ë¦¬ëœ ì´ íŒŒì¼ ìˆ˜

        for batch_result in self.large_project_analyzer.analyze_large_project(  # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ê¸° ì‹¤í–‰
            project_path, optimized_ast_analysis,             # í”„ë¡œì íŠ¸ ê²½ë¡œì™€ ë¶„ì„ í•¨ìˆ˜
            lambda msg, prog: progress_callback.update(f"Large project: {msg}", 15 + (prog * 0.6))  # ì§„í–‰ë¥  ì½œë°±
        ):
            all_analyses.extend(batch_result)                 # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— í•©ë³‘
            total_processed += len(batch_result)              # ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ ëˆ„ì 

            # ì£¼ê¸°ì  ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if total_processed % 500 == 0:                    # 500íŒŒì¼ë§ˆë‹¤
                progress_callback.update(f"Processed {total_processed}/{len(project_files)} files",  # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                                       15 + (total_processed / len(project_files)) * 60)

        progress_callback.update("Completing large project analysis", 80)  # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ

        # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë‹¨ìˆœí™”ëœ í†µí•© ì‚¬ìš©
        integrated_data = self._integrate_large_project_data(all_analyses, progress_callback)

        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë§¤ìš° í° í”„ë¡œì íŠ¸ëŠ” í’ˆì§ˆ ë©”íŠ¸ë¦­ ê±´ë„ˆë›°ê¸°
        quality_metrics = []                                  # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        if self.metrics_engine and len(project_files) < 5000:  # 5ì²œ íŒŒì¼ ë¯¸ë§Œì¼ ë•Œë§Œ
            progress_callback.update("Calculating quality metrics (subset)", 85)  # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° (ìƒ˜í”Œë§)
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ìœ„í•´ ìƒ˜í”Œë§Œ ë¶„ì„
            sample_size = min(1000, len(all_analyses))        # ìµœëŒ€ 1000ê°œ ìƒ˜í”Œ í¬ê¸°
            sample_analyses = all_analyses[:sample_size]      # ì•ìª½ ìƒ˜í”Œ ì„ íƒ
            quality_metrics = self._calculate_quality_metrics_sample(integrated_data, sample_analyses, progress_callback)

        # í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì¡°ë¦½
        progress_callback.update("Assembling results with pagination", 95)  # í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ê²°ê³¼ ì¡°ë¦½
        analysis_result = self._assemble_large_project_result(
            project_path, integrated_data, quality_metrics, start_time, progress_callback
        )

        return analysis_result                                # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    def _integrate_large_project_data(self, all_analyses: List[FileAnalysis],
                                     progress_callback: ProgressCallback) -> Dict:
        """ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ë‹¨ìˆœí™”ëœ ë°ì´í„° í†µí•©"""
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë³µì¡í•œ ê´€ê³„ ë¶„ì„ê³¼ ìˆœí™˜ ê²€ì¶œ ìƒëµ

        packages = {}                                         # íŒ¨í‚¤ì§€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        modules = []                                          # ëª¨ë“ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        classes = []                                          # í´ë˜ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        methods = []                                          # ë©”ì„œë“œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        fields = []                                           # í•„ë“œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        relationships = []                                    # ê´€ê³„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ë‹¨ìˆœí™”)

        for analysis in all_analyses:                         # ê° íŒŒì¼ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„° êµ¬ì¡°ë¡œ ë³€í™˜ (ë‹¨ìˆœí™”)
            module_info = ModuleInfo(
                id=create_module_id(analysis.file_path),      # ëª¨ë“ˆ ê³ ìœ  ID ìƒì„±
                name=Path(analysis.file_path).stem,           # íŒŒì¼ëª…ì—ì„œ ëª¨ë“ˆëª… ì¶”ì¶œ
                file_path=analysis.file_path,                 # íŒŒì¼ ì „ì²´ ê²½ë¡œ
                classes=analysis.classes,                     # í´ë˜ìŠ¤ ëª©ë¡ (ì´ë¯¸ ë³€í™˜ë¨)
                functions=analysis.functions,                 # í•¨ìˆ˜ ëª©ë¡
                imports=analysis.imports,                     # ì„í¬íŠ¸ ëª©ë¡
                loc=analysis.lines_of_code                    # ì½”ë“œ ë¼ì¸ ìˆ˜
            )
            modules.append(module_info)                       # ëª¨ë“ˆ ëª©ë¡ì— ì¶”ê°€

            # í´ë˜ìŠ¤ì™€ ë©”ì„œë“œ ì¶”ê°€ (ë‹¨ìˆœí™”)
            classes.extend(analysis.classes)                 # í´ë˜ìŠ¤ ëª©ë¡ í™•ì¥
            methods.extend(analysis.methods)                 # ë©”ì„œë“œ ëª©ë¡ í™•ì¥
            fields.extend(analysis.fields)                   # í•„ë“œ ëª©ë¡ í™•ì¥

        return {
            'packages': list(packages.values()),             # íŒ¨í‚¤ì§€ ëª©ë¡
            'modules': modules,                               # ëª¨ë“ˆ ëª©ë¡
            'classes': classes,                               # í´ë˜ìŠ¤ ëª©ë¡
            'methods': methods,                               # ë©”ì„œë“œ ëª©ë¡
            'fields': fields,                                 # í•„ë“œ ëª©ë¡
            'relationships': relationships,                   # ê´€ê³„ ëª©ë¡ (ë‹¨ìˆœí™”ë¨)
            'cycles': [],                                     # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ëŠ” ìˆœí™˜ ê²€ì¶œ ìƒëµ
            'metrics': {                                      # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì •ë³´
                'entity_counts': {                            # ì—”í‹°í‹° ê°œìˆ˜ í†µê³„
                    'modules': len(modules),                  # ëª¨ë“ˆ ìˆ˜
                    'classes': len(classes),                  # í´ë˜ìŠ¤ ìˆ˜
                    'methods': len(methods),                  # ë©”ì„œë“œ ìˆ˜
                    'fields': len(fields)                     # í•„ë“œ ìˆ˜
                }
            }
        }

    def _calculate_quality_metrics_sample(self, integrated_data: Dict,
                                        sample_analyses: List[FileAnalysis],
                                        progress_callback: ProgressCallback) -> List[QualityMetrics]:
        """ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œ ê¸°ë°˜ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ë©”ëª¨ë¦¬ì™€ ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ ì „ì²´ê°€ ì•„ë‹Œ ìƒ˜í”Œë§Œ ë¶„ì„

        if not self.metrics_engine:                          # ë©”íŠ¸ë¦­ ì—”ì§„ì´ ì—†ìœ¼ë©´
            return []                                         # ë¹ˆ ëª©ë¡ ë°˜í™˜

        sample_metrics = []                                   # ìƒ˜í”Œ ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ì¥

        # ë©”ëª¨ë¦¬ì™€ ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ í•˜ìœ„ ì§‘í•©ë§Œ ë¶„ì„
        for i, module in enumerate(integrated_data.get('modules', [])[:100]):  # ìµœëŒ€ 100ê°œ ëª¨ë“ˆë§Œ
            try:
                # ë‹¨ìˆœí™”ëœ í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰
                quality_metric = QualityMetrics(
                    entity_id=module.id,                     # ëª¨ë“ˆ ê³ ìœ  ID
                    entity_type=EntityType.MODULE,           # ì—”í‹°í‹° íƒ€ì… (ëª¨ë“ˆ)
                    cyclomatic_complexity=5,                 # ìˆœí™˜ ë³µì¡ë„ (ë‹¨ìˆœí™”ë¨)
                    lines_of_code=module.loc,                # ì½”ë“œ ë¼ì¸ ìˆ˜
                    quality_grade="B"                        # ê¸°ë³¸ í’ˆì§ˆ ë“±ê¸‰
                )
                sample_metrics.append(quality_metric)        # ë©”íŠ¸ë¦­ ëª©ë¡ì— ì¶”ê°€

            except Exception as e:                            # ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨ ì‹œ
                self.logger.warning(f"Failed to calculate metrics for {module.id}: {e}")  # ê²½ê³  ë¡œê·¸
                continue                                      # ë‹¤ìŒ ëª¨ë“ˆë¡œ ê³„ì†

        return sample_metrics                                 # ê³„ì‚°ëœ ìƒ˜í”Œ ë©”íŠ¸ë¦­ ë°˜í™˜

    def _assemble_large_project_result(self, project_path: str, integrated_data: Dict,
                                      quality_metrics: List[QualityMetrics],
                                      start_time: float, progress_callback: ProgressCallback) -> AnalysisResult:
        """í˜ì´ì§€ë„¤ì´ì…˜ì„ ì§€ì›í•˜ëŠ” ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ê²°ê³¼ ì¡°ë¦½"""
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œì„ ìœ„í•´ ê²°ê³¼ ë°ì´í„°ì— ì œí•œì„ ë‘ì–´ ì¡°ë¦½

        end_time = time.time()                                # ë¶„ì„ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        duration = end_time - start_time                     # ì´ ë¶„ì„ ì†Œìš” ì‹œê°„ ê³„ì‚°

        # í”„ë¡œì íŠ¸ ì •ë³´ ìƒì„±
        project_info = ProjectInfo(
            name=os.path.basename(project_path),             # í”„ë¡œì íŠ¸ ì´ë¦„ (í´ë”ëª…)
            path=project_path,                                # í”„ë¡œì íŠ¸ ì „ì²´ ê²½ë¡œ
            analyzed_at=datetime.now().isoformat(),          # ë¶„ì„ ì™„ë£Œ ì‹œê° (ISO í˜•ì‹)
            total_files=self.total_files,                    # ë¶„ì„ëœ ì´ íŒŒì¼ ìˆ˜
            analysis_duration_seconds=duration,              # ë¶„ì„ ì†Œìš” ì‹œê°„ (ì´ˆ)
            python_version=f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",  # Python ë²„ì „
            analysis_options=vars(self.options)              # ë¶„ì„ ì˜µì…˜ ì„¤ì •ê°’ë“¤
        )

        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ë¥¼ í¬í•¨í•œ ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±
        dependency_graph = DependencyGraph(
            packages=integrated_data['packages'],            # íŒ¨í‚¤ì§€ ëª©ë¡ (ì œí•œ ì—†ìŒ)
            modules=integrated_data['modules'][:1000],       # ëª¨ë“ˆì„ ë©”ëª¨ë¦¬ì—ì„œ ì œí•œ (ìµœëŒ€ 1000ê°œ)
            classes=integrated_data['classes'][:2000],       # í´ë˜ìŠ¤ ì œí•œ (ìµœëŒ€ 2000ê°œ)
            methods=integrated_data['methods'][:5000],       # ë©”ì„œë“œ ì œí•œ (ìµœëŒ€ 5000ê°œ)
            fields=integrated_data['fields'][:5000]          # í•„ë“œ ì œí•œ (ìµœëŒ€ 5000ê°œ)
        )

        # ìµœì¢… ê²°ê³¼ ìƒì„±
        result = AnalysisResult(
            analysis_id=self.current_analysis_id,            # ê³ ìœ  ë¶„ì„ ID
            project_info=project_info,                       # í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
            dependency_graph=dependency_graph,               # ì œí•œëœ ì˜ì¡´ì„± ê·¸ë˜í”„
            relationships=integrated_data['relationships'][:1000],  # ê´€ê³„ ì œí•œ (ìµœëŒ€ 1000ê°œ)
            quality_metrics=quality_metrics,                 # í’ˆì§ˆ ë©”íŠ¸ë¦­ (ìƒ˜í”Œë§ë¨)
            metrics=integrated_data['metrics'],              # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì •ë³´
            cycles=integrated_data['cycles']                  # ìˆœí™˜ ì˜ì¡´ì„± (ëŒ€ê·œëª¨ì—ì„œëŠ” ë¹ˆ ëª©ë¡)
        )

        # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ ë¡œê·¸
        self.logger.info(f"Large project analysis completed in {duration:.2f} seconds")  # ë¶„ì„ ì†Œìš” ì‹œê°„
        self.logger.info(f"Analyzed {len(integrated_data['modules'])} modules, "          # ì „ì²´ ë¶„ì„ëœ ìš”ì†Œ ìˆ˜
                        f"{len(integrated_data['classes'])} classes")

        return result                                         # ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    
    # ========= ê³µí†µ ë¡œì§ =========

    def _save_analysis_cache(self, project_path: str, project_files: List[str],
                            analysis_result: AnalysisResult):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ì—¬ ë‹¤ìŒë²ˆ ë¶„ì„ ì†ë„ í–¥ìƒ"""
        # íŒŒì¼ ë³€ê²½ ê°ì§€ì™€ ìºì‹œ ë¬´íš¨í™”ë¥¼ í†µí•œ ì¦ë¶„ ë¶„ì„ ì§€ì›
        try:
            cache_id = self.cache_manager.generate_cache_key(project_path, vars(self.options))  # ê³ ìœ  ìºì‹œ í‚¤ ìƒì„±

            # ë¶„ì„ëœ ëª¨ë“  íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„° ìƒì„±
            file_metadata = {}                                # íŒŒì¼ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            for file_path in project_files:                  # ê° ë¶„ì„ëœ íŒŒì¼ì— ëŒ€í•´
                if os.path.exists(file_path):                 # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´
                    file_metadata[file_path] = FileMetadata.from_file(file_path)  # ë©”íƒ€ë°ì´í„° ìƒì„±

            # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
            cache = AnalysisCache(
                cache_id=cache_id,                            # ê³ ìœ  ìºì‹œ ID
                project_path=project_path,                    # í”„ë¡œì íŠ¸ ê²½ë¡œ
                created_at=datetime.now(),                    # ìºì‹œ ìƒì„± ì‹œê°„
                expires_at=datetime.now() + timedelta(days=7),  # ìºì‹œ ë§Œë£Œ ì‹œê°„ (7ì¼)
                file_metadata=file_metadata,                  # íŒŒì¼ ë©”íƒ€ë°ì´í„°
                analysis_result=analysis_result               # ë¶„ì„ ê²°ê³¼
            )

            self.cache_manager.save_cache(cache)              # ìºì‹œ ë§¤ë‹ˆì €ë¥¼ í†µí•´ ì €ì¥
            self.logger.info(f"Analysis results cached with ID: {cache_id}")  # ìºì‹œ ì €ì¥ ì™„ë£Œ ë¡œê·¸

        except Exception as e:                                # ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ì‹œ
            self.logger.warning(f"Failed to save cache: {e}")  # ê²½ê³  ë¡œê·¸ (ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ)


def analyze_project(project_path: str,
                   options: AnalysisOptions = None,
                   progress_callback: ProgressCallback = None) -> AnalysisResult:
    """
    Python í”„ë¡œì íŠ¸ ë¶„ì„ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜

    Args:
        project_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        options: ë¶„ì„ ì˜µì…˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        progress_callback: ì„ íƒì  ì§„í–‰ë¥  ì½œë°±

    Returns:
        ì™„ì „í•œ ë¶„ì„ ê²°ê³¼
    """
    engine = AnalyzerEngine(options)                                                        # ë¶„ì„ ì—”ì§„ ìƒì„±
    return engine.analyze_project(project_path, progress_callback)                         # í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜