#!/usr/bin/env python3
"""
PyView FastAPI ÏÑúÎ≤Ñ
Python ÏùòÏ°¥ÏÑ± ÏãúÍ∞ÅÌôî Ïù∏ÌÑ∞ÎûôÌã∞Î∏å ÏÑúÎ≤Ñ
"""
import os
import sys
import asyncio
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List
import json

# Debug ÏÑ§Ï†ï
DEBUG_MODE = os.getenv('PYVIEW_DEBUG', 'false').lower() == 'true'

import uvicorn
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# Î≥µÏû°Ìïú Îç∞Î™® Îç∞Ïù¥ÌÑ∞ import
from demo_complex_data import create_complex_web_app_demo, create_microservices_demo

# pyview importÎ•º ÏúÑÌï¥ ÏÉÅÏúÑ ÎîîÎ†âÌÜ†Î¶¨Î•º Python pathÏóê Ï∂îÍ∞Ä
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from pyview.analyzer_engine import AnalyzerEngine
    from pyview.models import AnalysisResult
except ImportError as e:
    print(f"pyview Î™®Îìà import ÏóêÎü¨: {e}")
    print("pyview Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏñ¥ ÏûàÍ±∞ÎÇò Python pathÏóê ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî")
    sys.exit(1)

app = FastAPI(
    title="PyView API",
    description="Python ÏùòÏ°¥ÏÑ± Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî ÏÑúÎπÑÏä§",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# In-memory storage for demo (use database in production)
analyses: Dict[str, Dict] = {}
active_connections: Dict[str, List[WebSocket]] = {}

# Request/Response models
class AnalysisOptions(BaseModel):
    max_depth: int = 10
    exclude_patterns: List[str] = []
    include_stdlib: bool = False
    analysis_levels: List[str] = ["package", "module", "class"]
    enable_type_inference: bool = True
    max_workers: int = 4

class AnalysisRequest(BaseModel):
    project_path: str
    options: AnalysisOptions

class AnalysisResponse(BaseModel):
    analysis_id: str
    status: str
    message: str

class AnalysisStatusResponse(BaseModel):
    analysis_id: str
    status: str
    progress: float
    message: str
    created_at: str
    updated_at: str
    error: Optional[str] = None

class SearchRequest(BaseModel):
    query: str
    entity_type: Optional[str] = None
    analysis_id: Optional[str] = None

class SearchResult(BaseModel):
    name: str
    entity_type: str
    module_path: str
    file_path: str
    line_number: Optional[int] = None
    description: Optional[str] = None
    is_in_cycle: Optional[bool] = False
    cycle_severity: Optional[str] = None

class SearchResponse(BaseModel):
    query: str
    total_results: int
    results: List[SearchResult]

class DirectoryItem(BaseModel):
    name: str
    path: str
    is_directory: bool
    has_python_files: bool = False

class BrowseDirectoryResponse(BaseModel):
    current_path: str
    parent_path: Optional[str]
    directories: List[DirectoryItem]

class QualityMetricsResponse(BaseModel):
    entity_id: str
    entity_type: str
    cyclomatic_complexity: int
    lines_of_code: int
    afferent_coupling: int
    efferent_coupling: int
    instability: float
    maintainability_index: float
    technical_debt_ratio: float
    quality_grade: str
    is_in_cycle: bool = False
    cycle_severity: Optional[str] = None
    cycle_partners: List[str] = []
    cycle_type: Optional[str] = None

# ÏàúÌôò Ï∞∏Ï°∞ Í¥ÄÎ†® Î™®Îç∏Îì§
class CyclePath(BaseModel):
    nodes: List[str]
    relationship_type: str  # 'import' or 'call'
    strength: float

class CycleMetrics(BaseModel):
    cycle_length: int
    total_strength: float
    average_strength: float
    severity: str  # 'low', 'medium', 'high'

class CyclicDependencyResponse(BaseModel):
    cycle_id: str
    entities: List[str]
    relationship_type: str  # 'import' or 'call'
    severity: str  # 'low', 'medium', 'high'
    paths: List[CyclePath]
    metrics: CycleMetrics
    description: str

class CycleDetectionResponse(BaseModel):
    analysis_id: str
    cycles: List[CyclicDependencyResponse]
    total_cycles: int
    cycle_statistics: Dict[str, int]

def check_entity_in_cycles(entity_id: str, analysis_results: Dict) -> tuple[bool, Optional[str]]:
    """ÏóîÌã∞Ìã∞Í∞Ä ÏàúÌôò Ï∞∏Ï°∞Ïóê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏"""
    cycles = analysis_results.get("cycles", [])
    
    for cycle in cycles:
        if entity_id in cycle.get("entities", []):
            severity = cycle.get("severity", "medium")
            return True, severity
    
    return False, None

def build_cycle_entity_map(analysis_results: Dict) -> Dict[str, Dict]:
    """ÏàúÌôò Ï∞∏Ï°∞Ïóê Ìè¨Ìï®Îêú Î™®Îì† ÏóîÌã∞Ìã∞Ïùò ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Îßµ ÏÉùÏÑ±"""
    cycle_map = {}
    cycles = analysis_results.get("cycles", [])
    
    for cycle in cycles:
        severity = cycle.get("severity", "medium")
        cycle_type = cycle.get("relationship_type", "import")
        entities = cycle.get("entities", [])
        
        for entity_id in entities:
            # Îçî ÎÜíÏùÄ Ïã¨Í∞ÅÎèÑÎ°ú ÏóÖÎç∞Ïù¥Ìä∏
            if entity_id not in cycle_map or severity == "high":
                cycle_map[entity_id] = {
                    "severity": severity,
                    "partners": [e for e in entities if e != entity_id],
                    "cycle_type": cycle_type
                }
    
    return cycle_map

def create_analysis_record(analysis_id: str, request: AnalysisRequest) -> Dict:
    """Create a new analysis record"""
    now = datetime.now().isoformat()
    return {
        "analysis_id": analysis_id,
        "status": "pending",
        "progress": 0.0,
        "message": "Analysis queued",
        "created_at": now,
        "updated_at": now,
        "request": request.dict(),
        "results": None,
        "error": None
    }

def update_analysis_status(analysis_id: str, status: str, progress: float = None, 
                          message: str = None, error: str = None, results = None):
    """Update analysis status"""
    if analysis_id not in analyses:
        return
    
    record = analyses[analysis_id]
    record["status"] = status
    record["updated_at"] = datetime.now().isoformat()
    
    if progress is not None:
        record["progress"] = progress
    if message is not None:
        record["message"] = message
    if error is not None:
        record["error"] = error
    if results is not None:
        record["results"] = results

async def send_progress_update(analysis_id: str, stage: str, progress: float, 
                              message: str, current_file: str = None):
    """Send progress update via WebSocket"""
    if analysis_id not in active_connections:
        return
    
    update = {
        "analysis_id": analysis_id,
        "stage": stage,
        "progress": progress,
        "message": message,
        "current_file": current_file,
        "files_processed": int(progress * 100),
        "total_files": 100
    }
    
    # Send to all connected WebSocket clients for this analysis
    disconnected = []
    for ws in active_connections[analysis_id]:
        try:
            await ws.send_text(json.dumps(update))
        except:
            disconnected.append(ws)
    
    # Remove disconnected clients
    for ws in disconnected:
        active_connections[analysis_id].remove(ws)

async def run_analysis_task(analysis_id: str, request: AnalysisRequest):
    """Run analysis in background task"""
    try:
        update_analysis_status(analysis_id, "running", 0.0, "Starting analysis...")
        await send_progress_update(analysis_id, "initialization", 0.1, "Initializing analyzer")
        await asyncio.sleep(0.2)

        await send_progress_update(analysis_id, "validation", 0.15, "Validating project path")

        # Check project path validation
        project_path_str = request.project_path.strip()

        # Check if path is absolute
        if not os.path.isabs(project_path_str):
            raise ValueError(f"Project path must be an absolute path: {project_path_str}")

        # Check if project path exists
        project_path = Path(project_path_str)
        if not project_path.exists():
            raise ValueError(f"Project path does not exist: {project_path_str}")

        # Check if it's a directory
        if not project_path.is_dir():
            raise ValueError(f"Project path must be a directory: {project_path_str}")

        await send_progress_update(analysis_id, "scanning", 0.25, "Scanning Python files")
        await asyncio.sleep(0.15)

        # Check if it's a Python project (has .py files) with exclusion patterns
        print(f"üîç Starting .py file scan in: {project_path_str}", flush=True)

        # For root path '/', limit the search to avoid system directories
        if project_path_str == '/':
            print("‚ö†Ô∏è Root path detected, limiting search to common user directories", flush=True)
            search_paths = [
                Path('/Users'),
                Path('/home'),
                Path('/opt'),
                Path('/usr/local/src'),
                Path('/tmp')
            ]
            all_py_files = []
            for search_path in search_paths:
                if search_path.exists():
                    try:
                        py_files_in_path = list(search_path.rglob("*.py"))
                        all_py_files.extend(py_files_in_path)
                        print(f"üìÅ Found {len(py_files_in_path)} .py files in {search_path}", flush=True)
                    except (PermissionError, OSError) as e:
                        print(f"‚ö†Ô∏è Skipped {search_path}: {e}", flush=True)
                        continue
        else:
            all_py_files = list(project_path.rglob("*.py"))

        print(f"üìä Total .py files found: {len(all_py_files)}", flush=True)

        # Apply basic exclusion patterns early to avoid timeout issues
        basic_exclude_patterns = request.options.exclude_patterns + [
            "*GoogleDrive*", "*Library*", "__pycache__", ".git", "node_modules", ".venv"
        ]
        print(f"üîç Exclude patterns: {basic_exclude_patterns}", flush=True)

        py_files = []
        excluded_count = 0
        for py_file in all_py_files:
            file_path_str = str(py_file)

            # Check if any pattern matches
            excluded = False
            for pattern in basic_exclude_patterns:
                if pattern in file_path_str:
                    excluded = True
                    excluded_count += 1
                    if "GoogleDrive" in file_path_str:
                        print(f"üö´ Excluded GoogleDrive file: {file_path_str}", flush=True)
                    break

            if not excluded:
                py_files.append(py_file)

        print(f"üìä Total .py files found: {len(all_py_files)}, Excluded: {excluded_count}, Remaining: {len(py_files)}", flush=True)

        if not py_files:
            raise ValueError(f"No Python files found in project path: {project_path_str}")

        await send_progress_update(analysis_id, "analyzing", 0.35, f"Found {len(py_files)} Python files")
        await asyncio.sleep(0.15)

        await send_progress_update(analysis_id, "dependencies", 0.45, "Analyzing dependencies")
        
        # Create analysis options
        from pyview.analyzer_engine import AnalysisOptions, ProgressCallback
        
        try:
            # Create options object with user settings
            # Validate max_depth range (should match frontend validation)
            user_max_depth = request.options.max_depth
            if user_max_depth < 1:
                user_max_depth = 1
            elif user_max_depth > 50:
                user_max_depth = 50

            options = AnalysisOptions(
                max_depth=user_max_depth,  # Use validated user setting
                exclude_patterns=basic_exclude_patterns,  # Use same patterns as initial scan
                include_stdlib=request.options.include_stdlib,  # Use user setting
                analysis_levels=["package", "module", "class", "method"],  # Include more levels for better cycle detection
                enable_type_inference=request.options.enable_type_inference,  # Use user setting
                enable_quality_metrics=True,  # Enable quality metrics (now optimized)
                enable_caching=False,  # Disable caching for now
                max_workers=1  # Use single worker to prevent issues
            )
            
            # Create analyzer engine with options
            engine = AnalyzerEngine(options)
            
            # Create progress callback that converts sync to async
            def sync_progress_callback(data: dict):
                stage = data.get('stage', 'processing')
                progress = data.get('progress', 0) / 100.0  # Convert to 0-1 range
                message = data.get('message', stage)
                current_file = data.get('current_file')
                
                # We can't await in sync callback, so we'll schedule it
                asyncio.create_task(
                    send_progress_update(analysis_id, stage, progress, message, current_file)
                )
                update_analysis_status(analysis_id, "running", progress, message)
            
            progress_callback = ProgressCallback(sync_progress_callback)
            
            # Check if this is a request for complex demo data
            project_path_str = str(project_path).lower()
            if "demo" in project_path_str or "complex" in project_path_str:
                # Mock ÏßÑÌñâÎ•† Îã®Í≥Ñ for demo
                await send_progress_update(analysis_id, "setup", 0.55, "Setting up analysis engine")
                await asyncio.sleep(0.3)

                await send_progress_update(analysis_id, "processing", 0.65, "Processing AST")
                await asyncio.sleep(0.1)

                await send_progress_update(analysis_id, "relationships", 0.75, "Building dependency relationships")
                await asyncio.sleep(0.2)

                await send_progress_update(analysis_id, "cycles", 0.85, "Detecting circular dependencies")
                await asyncio.sleep(0.2)

                await send_progress_update(analysis_id, "finalizing", 0.95, "Finalizing results")
                await asyncio.sleep(0.1)

                results = create_complex_web_app_demo()
            elif "microservice" in project_path_str:
                # Mock ÏßÑÌñâÎ•† Îã®Í≥Ñ for microservice demo
                await send_progress_update(analysis_id, "setup", 0.55, "Setting up analysis engine")
                await asyncio.sleep(0.5)

                await send_progress_update(analysis_id, "processing", 0.65, "Processing microservices")
                await asyncio.sleep(0.1)

                await send_progress_update(analysis_id, "finalizing", 0.95, "Finalizing results")
                await asyncio.sleep(0.3)

                results = create_microservices_demo()
            else:
                # Use actual analysis engine with timeout
                try:
                    print(f"Starting analysis of: {project_path}")
                    if DEBUG_MODE:
                        with open('/tmp/pyview_debug.log', 'a') as f:
                            f.write(f"üîç SERVER DEBUG: Starting analysis of: {project_path}, include_stdlib: {request.options.include_stdlib}\n")

                    await send_progress_update(analysis_id, "engine_start", 0.55, f"Starting analysis engine for: {project_path.name}")
                    await asyncio.sleep(0.2)

                    await send_progress_update(analysis_id, "processing", 0.65, "Processing AST and dependencies")
                    await asyncio.sleep(0.2)

                    # Run the actual analysis using the analyzer engine
                    result = engine.analyze_project(str(project_path), progress_callback)

                    await send_progress_update(analysis_id, "finalizing", 0.95, "Finalizing analysis results")
                    await asyncio.sleep(0.1)
                    print(f"Analysis completed successfully")
                    
                    # Convert AnalysisResult to dictionary format
                    if hasattr(result, 'to_dict'):
                        results = result.to_dict()
                        modules_count = len(results.get('dependency_graph', {}).get('modules', []))
                        classes_count = len(results.get('dependency_graph', {}).get('classes', []))
                        print(f"Converted result to dict: {modules_count} modules, {classes_count} classes found")
                    else:
                        # If the result doesn't have to_dict, try to extract data manually
                        packages = getattr(result, 'packages', [])
                        modules = getattr(result, 'modules', [])
                        classes = getattr(result, 'classes', [])
                        methods = getattr(result, 'methods', [])
                        fields = getattr(result, 'fields', [])
                        dependencies = getattr(result, 'dependencies', [])
                        
                        results = {
                            "summary": {
                                "total_packages": len(packages),
                                "total_modules": len(modules),
                                "total_classes": len(classes),
                                "total_methods": len(methods),
                                "total_fields": len(fields),
                            },
                            "packages": packages,
                            "modules": modules,
                            "classes": classes,
                            "methods": methods,
                            "fields": fields,
                            "dependencies": dependencies
                        }
                        print(f"Manually extracted data: {len(modules)} modules, {len(dependencies)} dependencies")
                        
                except Exception as analysis_error:
                    print(f"Analysis failed with error: {analysis_error}")
                    import traceback
                    traceback.print_exc()
                    
                    # Create a basic analysis result with error info
                    results = {
                        "summary": {
                            "total_packages": 1,
                            "total_modules": 1,
                            "total_classes": 0,
                            "total_methods": 0,
                            "total_fields": 0,
                            "error": f"Analysis failed: {str(analysis_error)}"
                        },
                        "packages": [{"package_id": "pkg_error", "name": f"Failed: {project_path.name}", "modules": ["error_module"]}],
                        "modules": [{
                            "module_id": "mod_error", 
                            "name": f"Error analyzing {project_path.name}", 
                            "file_path": str(project_path),
                            "error": str(analysis_error)
                        }],
                        "classes": [],
                        "methods": [],
                        "fields": [],
                        "dependencies": []
                    }
            
            update_analysis_status(analysis_id, "completed", 1.0, "Analysis completed successfully", results=results)
            await send_progress_update(analysis_id, "completed", 1.0, "Analysis completed successfully")
            
        except Exception as e:
            error_msg = f"Analysis failed: {str(e)}"
            update_analysis_status(analysis_id, "failed", None, error_msg, error=error_msg)
            await send_progress_update(analysis_id, "failed", 0.0, error_msg)
    
    except Exception as e:
        error_msg = f"Analysis task failed: {str(e)}"
        update_analysis_status(analysis_id, "failed", None, error_msg, error=error_msg)
        if analysis_id in active_connections:
            await send_progress_update(analysis_id, "failed", 0.0, error_msg)

# API Routes
@app.get("/")
async def root():
    """Health check endpoint"""
    return {"message": "PyView API Server", "version": "1.0.0", "status": "running"}

@app.get("/api/health")
async def health_check():
    """API Health check endpoint"""
    return {"status": "running", "message": "PyView API Server", "version": "1.0.0"}

@app.get("/api/browse-directory", response_model=BrowseDirectoryResponse)
async def browse_directory(path: str = None):
    """Browse directories on the local filesystem"""
    try:
        # If no path provided, use home directory
        if not path or path.strip() == "":
            path = str(Path.home())

        current_path = Path(path).expanduser().resolve()

        # Check if path exists and is a directory
        if not current_path.exists():
            raise HTTPException(status_code=404, detail="Path does not exist")

        if not current_path.is_dir():
            raise HTTPException(status_code=400, detail="Path is not a directory")

        # Get parent directory
        parent_path = str(current_path.parent) if current_path.parent != current_path else None

        # List directories in current path
        directories = []
        try:
            for item in sorted(current_path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower())):
                # Only include directories
                if item.is_dir():
                    # Skip hidden directories (starting with .)
                    if item.name.startswith('.'):
                        continue

                    # Check if directory has Python files
                    has_python_files = False
                    try:
                        # Quick check: see if there are any .py files in immediate children
                        has_python_files = any(
                            child.suffix == '.py'
                            for child in item.iterdir()
                            if child.is_file()
                        )
                    except (PermissionError, OSError):
                        pass

                    directories.append(DirectoryItem(
                        name=item.name,
                        path=str(item),
                        is_directory=True,
                        has_python_files=has_python_files
                    ))
        except PermissionError:
            raise HTTPException(status_code=403, detail="Permission denied to read directory")

        return BrowseDirectoryResponse(
            current_path=str(current_path),
            parent_path=parent_path,
            directories=directories
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error browsing directory: {str(e)}")

@app.post("/api/analyze", response_model=AnalysisResponse)
async def start_analysis(request: AnalysisRequest):
    """Start a new project analysis"""
    analysis_id = str(uuid.uuid4())
    
    # Create analysis record
    analyses[analysis_id] = create_analysis_record(analysis_id, request)
    
    # Start background task
    asyncio.create_task(run_analysis_task(analysis_id, request))
    
    return AnalysisResponse(
        analysis_id=analysis_id,
        status="pending",
        message="Analysis started"
    )

@app.get("/api/analysis/{analysis_id}/status", response_model=AnalysisStatusResponse)
async def get_analysis_status(analysis_id: str):
    """Get analysis status"""
    if analysis_id not in analyses:
        # Create a dummy record for missing analysis IDs to prevent 404 loops
        print(f"Analysis ID {analysis_id} not found, creating dummy record")
        analyses[analysis_id] = {
            "analysis_id": analysis_id,
            "status": "failed",
            "progress": 0.0,
            "message": "Analysis session lost (server restart)",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "error": "Analysis session was lost due to server restart. Please start a new analysis.",
            "results": None
        }
    
    record = analyses[analysis_id]
    return AnalysisStatusResponse(**record)

@app.get("/api/analysis/{analysis_id}/results")
async def get_analysis_results(analysis_id: str):
    """Get analysis results"""
    if analysis_id not in analyses:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    record = analyses[analysis_id]
    if record["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis not completed")
    
    return record["results"]

@app.get("/api/analyses", response_model=List[AnalysisStatusResponse])
async def get_all_analyses():
    """Get all analyses"""
    return [AnalysisStatusResponse(**record) for record in analyses.values()]

@app.delete("/api/analysis/{analysis_id}")
async def delete_analysis(analysis_id: str):
    """Delete analysis"""
    if analysis_id not in analyses:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    del analyses[analysis_id]
    if analysis_id in active_connections:
        del active_connections[analysis_id]
    
    return {"message": "Analysis deleted"}

@app.post("/api/search", response_model=SearchResponse)
async def search_entities(request: SearchRequest):
    """Search entities across analyses"""
    results = []
    
    # Search across all completed analyses or specific analysis if provided
    target_analyses = []
    if request.analysis_id and request.analysis_id in analyses:
        if analyses[request.analysis_id]["status"] == "completed":
            target_analyses = [analyses[request.analysis_id]]
    else:
        # Search across all completed analyses
        target_analyses = [record for record in analyses.values() 
                          if record["status"] == "completed"]
    
    if request.query:
        query_lower = request.query.lower()
        
        for analysis_record in target_analyses:
            analysis_results = analysis_record.get("results")
            if not analysis_results:
                continue
            
            # ÏàúÌôò Ï∞∏Ï°∞ Îßµ ÏÉùÏÑ±
            cycle_map = build_cycle_entity_map(analysis_results)
                
            # Search in modules
            for module in analysis_results.get("dependency_graph", {}).get("modules", []):
                if query_lower in module.get("name", "").lower():
                    module_id = module.get("id", module.get("name", ""))
                    is_in_cycle = module_id in cycle_map
                    cycle_severity = cycle_map.get(module_id)
                    
                    results.append(SearchResult(
                        name=module.get("name", ""),
                        entity_type="module",
                        module_path=module.get("name", ""),
                        file_path=module.get("file_path", ""),
                        line_number=1,
                        description=f"Module: {module.get('name', '')}",
                        is_in_cycle=is_in_cycle,
                        cycle_severity=cycle_severity
                    ))
            
            # Search in classes
            for cls in analysis_results.get("dependency_graph", {}).get("classes", []):
                if query_lower in cls.get("name", "").lower():
                    class_id = cls.get("id", cls.get("name", ""))
                    is_in_cycle = class_id in cycle_map
                    cycle_severity = cycle_map.get(class_id)
                    
                    results.append(SearchResult(
                        name=cls.get("name", ""),
                        entity_type="class",
                        module_path=cls.get("module", ""),
                        file_path=cls.get("file_path", ""),
                        line_number=cls.get("line_number", 1),
                        description=f"Class: {cls.get('name', '')} in {cls.get('module', '')}",
                        is_in_cycle=is_in_cycle,
                        cycle_severity=cycle_severity
                    ))
            
            # Search in methods
            for method in analysis_results.get("dependency_graph", {}).get("methods", []):
                if query_lower in method.get("name", "").lower():
                    method_id = method.get("id", method.get("name", ""))
                    is_in_cycle = method_id in cycle_map
                    cycle_severity = cycle_map.get(method_id)
                    
                    results.append(SearchResult(
                        name=method.get("name", ""),
                        entity_type="method",
                        module_path=method.get("class_name", ""),
                        file_path=method.get("file_path", ""),
                        line_number=method.get("line_number", 1),
                        description=f"Method: {method.get('name', '')} in {method.get('class_name', '')}",
                        is_in_cycle=is_in_cycle,
                        cycle_severity=cycle_severity
                    ))
    
    return SearchResponse(
        query=request.query,
        total_results=len(results),
        results=results
    )

@app.get("/api/analysis/{analysis_id}/quality-metrics", response_model=List[QualityMetricsResponse])
async def get_quality_metrics(analysis_id: str):
    """Get quality metrics for analysis results"""
    if analysis_id not in analyses:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    record = analyses[analysis_id]
    if record["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis not completed")
    
    # Get actual quality metrics from analysis results
    analysis_results = record.get("results")
    if not analysis_results:
        return []
    
    quality_metrics = []
    
    # ÏàúÌôò Ï∞∏Ï°∞ Îßµ ÏÉùÏÑ±
    cycle_map = build_cycle_entity_map(analysis_results)
    
    # Extract quality metrics from actual analysis results
    actual_metrics = analysis_results.get("quality_metrics", [])
    if actual_metrics:
        # Use actual quality metrics if available
        print(f"‚úÖ Using {len(actual_metrics)} actual quality metrics from analysis engine")
        for metric in actual_metrics:
            entity_id = metric.get("entity_id", "unknown")
            is_in_cycle = entity_id in cycle_map
            cycle_info = cycle_map.get(entity_id, {})

            # Get coupling metrics from analysis results
            coupling_metrics = analysis_results.get("metrics", {}).get("coupling_metrics", {}).get(entity_id, {})

            quality_metrics.append(QualityMetricsResponse(
                entity_id=entity_id,
                entity_type=metric.get("entity_type", "module"),
                cyclomatic_complexity=metric.get("cyclomatic_complexity", 0),
                lines_of_code=metric.get("lines_of_code", 0),
                afferent_coupling=coupling_metrics.get("afferent_coupling", 0),
                efferent_coupling=coupling_metrics.get("efferent_coupling", 0),
                instability=coupling_metrics.get("instability", 0.0),
                maintainability_index=metric.get("maintainability_index", 0.0),
                technical_debt_ratio=0.0,  # TODO: Calculate from complexity metrics
                quality_grade=metric.get("quality_grade", "C"),
                is_in_cycle=is_in_cycle,
                cycle_severity=cycle_info.get("severity"),
                cycle_partners=cycle_info.get("partners", []),
                cycle_type=cycle_info.get("cycle_type")
            ))
    else:
        # Fallback: Generate basic quality metrics from modules and classes
        print("‚ö†Ô∏è  WARNING: Quality metrics engine disabled or failed. Generating fallback dummy metrics.")
        dependency_graph = analysis_results.get("dependency_graph", {})
        
        # Add metrics for modules
        for module in dependency_graph.get("modules", []):
            entity_id = module.get("id", module.get("name", "unknown"))
            is_in_cycle = entity_id in cycle_map
            cycle_info = cycle_map.get(entity_id, {})
            
            quality_metrics.append(QualityMetricsResponse(
                entity_id=entity_id,
                entity_type="module",
                cyclomatic_complexity=module.get("complexity", 5),
                lines_of_code=module.get("loc", 100),
                afferent_coupling=len(module.get("dependencies", [])),
                efferent_coupling=len([dep for dep in analysis_results.get("relationships", []) 
                                    if dep.get("from_entity") == module.get("id")]),
                instability=0.5,
                maintainability_index=75.0,
                technical_debt_ratio=0.1,
                quality_grade="B",
                is_in_cycle=is_in_cycle,
                cycle_severity=cycle_info.get("severity"),
                cycle_partners=cycle_info.get("partners", []),
                cycle_type=cycle_info.get("cycle_type")
            ))
        
        # Add metrics for classes (limit to first 50 for pagination testing)
        for cls in dependency_graph.get("classes", [])[:50]:
            entity_id = cls.get("id", cls.get("name", "unknown"))
            is_in_cycle = entity_id in cycle_map
            cycle_info = cycle_map.get(entity_id, {})
            
            quality_metrics.append(QualityMetricsResponse(
                entity_id=entity_id,
                entity_type="class",
                cyclomatic_complexity=len(cls.get("methods", [])) * 2,
                lines_of_code=len(cls.get("methods", [])) * 15,
                afferent_coupling=1,
                efferent_coupling=len(cls.get("methods", [])),
                instability=0.6,
                maintainability_index=70.0,
                technical_debt_ratio=0.15,
                quality_grade="B",
                is_in_cycle=is_in_cycle,
                cycle_severity=cycle_info.get("severity"),
                cycle_partners=cycle_info.get("partners", []),
                cycle_type=cycle_info.get("cycle_type")
            ))
    
    # Remove duplicates based on entity_id
    seen_entity_ids = set()
    deduplicated_metrics = []
    duplicates_removed = 0

    for metric in quality_metrics:
        if metric.entity_id not in seen_entity_ids:
            seen_entity_ids.add(metric.entity_id)
            deduplicated_metrics.append(metric)
        else:
            duplicates_removed += 1
            print(f"REMOVED DUPLICATE: entity_id={metric.entity_id}, type={metric.entity_type}")

    if duplicates_removed > 0:
        print(f"Quality Metrics: Removed {duplicates_removed} duplicate entries")
        print(f"Final count: {len(deduplicated_metrics)} unique entities")

    return deduplicated_metrics

@app.get("/api/cache/stats")
async def get_cache_stats():
    """Get cache statistics"""
    # This would integrate with the actual CacheManager
    return {
        "cache_count": 5,
        "total_size_mb": 12.5,
        "max_size_mb": 500.0,
        "memory_cache_count": 2,
        "cache_dir": "~/.pyview_cache"
    }

@app.delete("/api/cache")
async def clear_cache():
    """Clear all analysis caches"""
    # This would call cache_manager.clear_all_caches()
    return {"message": "Cache cleared successfully"}

@app.websocket("/ws/progress/{analysis_id}")
async def websocket_progress(websocket: WebSocket, analysis_id: str):
    """WebSocket endpoint for real-time progress updates"""
    await websocket.accept()
    
    if analysis_id not in active_connections:
        active_connections[analysis_id] = []
    active_connections[analysis_id].append(websocket)
    
    try:
        while True:
            # Keep connection alive
            await websocket.receive_text()
    except WebSocketDisconnect:
        active_connections[analysis_id].remove(websocket)
        if not active_connections[analysis_id]:
            del active_connections[analysis_id]

# Serve static files (frontend build)
frontend_static = Path(__file__).parent / "static"
if frontend_static.exists():
    app.mount("/", StaticFiles(directory=str(frontend_static), html=True), name="static")

if __name__ == "__main__":
    print("Starting PyView FastAPI Server...")
    print("Backend API: http://localhost:8000")
    print("Frontend UI: http://localhost:3000")
    print("API Docs: http://localhost:8000/docs")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )